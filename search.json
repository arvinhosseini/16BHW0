[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Heat Diffusion/index.html",
    "href": "posts/Heat Diffusion/index.html",
    "title": "Simulation of Heat Diffusion",
    "section": "",
    "text": "Hello! In this blog post we will go over how to simulate heat diffusion using four different methods. These methods consists of traditional matrix multiplication, using a sparse matrix, direct operation via numpy, and just-in-time complilation using JAX. We will compare how efficent these methods are by comparing the run time for each method. We hope to find a decrease in run time going from each method to the next."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#intro",
    "href": "posts/Heat Diffusion/index.html#intro",
    "title": "Simulation of Heat Diffusion",
    "section": "",
    "text": "Hello! In this blog post we will go over how to simulate heat diffusion using four different methods. These methods consists of traditional matrix multiplication, using a sparse matrix, direct operation via numpy, and just-in-time complilation using JAX. We will compare how efficent these methods are by comparing the run time for each method. We hope to find a decrease in run time going from each method to the next."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#set-up",
    "href": "posts/Heat Diffusion/index.html#set-up",
    "title": "Simulation of Heat Diffusion",
    "section": "Set Up",
    "text": "Set Up\nWe will first set our constants for the heat diffusion problem.\n\nN = 101\nepsilon = 0.2\n\nWe will then show the initial phase of the heat diffusion by placing one unit of heat in a middle of an NxN grid.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n&lt;matplotlib.image.AxesImage at 0x112d7c9d0&gt;\n\n\n\n\n\nAs we can see have a 101x101 grid with the yellow dot representing the initial unit of heat."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#matrix-multiplication",
    "href": "posts/Heat Diffusion/index.html#matrix-multiplication",
    "title": "Simulation of Heat Diffusion",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nWe will now use Matrix Multiplication and the definitions of the heat diffusion equations to simulate the diffusion of this point over 2700 iterations.\nWe first need to define the function advance_time_matvecmul that calculates the next matrix for each iteration.\n\nimport inspect\nfrom heat_equations import advance_time_matvecmul\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nWe can see that the function takes in A, u, and epsilon as its parameters. A represetns the finite difference matrix that we will define below. u is the grid at the next step, so each time the function is called we will return a new grid u, that shows the next step of the grid. Finally epsilon is the stability constant.\nWe define A as the following, using the function get_A(N).\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"Generates the 2D finite difference matrix A for the heat equation\n    Args:\n        N: Size of the grid (N x N)\n\n    Returns:\n        2D finite difference matrix A\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\nWe can see that A is also an NxN, similarily to u, that defines what constants is needed to find the next step of the grid. Therefore we can see that the main computation in this method is multiplying the NxN matrices A and u for each iteration. T\nhis is a very costly computation. We will run the following script to find the results of the simulation and also inspect the runtime of this method.\n\nimport heat_equation as eq\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Parameters\nN = 101\nepsilon = 0.2\niterations = 2700\nvisualization_interval = 300\n\n# Initialize the grid\nu = np.zeros((N, N))\nu[int(N/2), int(N/2)] = 1.0\n\n# Initialize list to store intermediate solutions\nintermediate_solutions = []\n\n# Get matrix A\nA = eq.get_A(N)\n\n# Run simulation\nstart_time_simulation = time.time()\nfor i in range(iterations):\n    # Advance the simulation by one timestep\n    u = eq.advance_time_matvecmul(A, u, epsilon)\n    \n    # Store intermediate solutions every 300 iterations\n    if (i+1) % visualization_interval == 0:\n        intermediate_solutions.append(u)\n\nend_time_simulation = time.time()\nprint(f\"Total time taken for the simulation using Matrix Multiplication: {end_time_simulation - start_time_simulation} seconds\")\n\n# Visualize the diffusion of heat every 300 iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\n\nfor i in range(3):\n    for j in range(3):\n        index = i * 3 + j\n        iteration_number = (index + 1) * visualization_interval  # Calculate iteration number dynamically\n        axes[i, j].imshow(intermediate_solutions[index])\n        axes[i, j].set_title(f\"Iteration {iteration_number}\")\n\nplt.tight_layout()\nplt.show()\n\nTotal time taken for the simulation using Matrix Multiplication: 137.2656331062317 seconds\n\n\n\n\n\nFrom the output we can see how the heat diffusion looks after 300 iterations, 600 iterations, and so on until iteration 2700. We can see that the heat spreads pretty evenly with the center retaining the most heat. The visualization is color mapped so we can see that yellow indicates a higher amount of heat while blue indicates little to no heat.\nWe can also see that this method took about 137 seconds to complete this simulation using the matrix multiplication method. We will try to shorten that run time using the next method."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#sparse-matrix",
    "href": "posts/Heat Diffusion/index.html#sparse-matrix",
    "title": "Simulation of Heat Diffusion",
    "section": "Sparse Matrix",
    "text": "Sparse Matrix\nThe next method we will use a spare matrix to represent the A matrix instead of our traditional matrix. Sparse matrices will significantly help with our efficiency because they only store the non zero values in a matrix. Since our matrix A has a lot of zero values this will hopefully be effective in reducing the computation time for the simulation.\nLet’s show what the get_sparse_A function looks like.\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A)) \n\ndef get_sparse_A(N):\n    \"\"\"Generates the sparse 2D finite difference matrix A for the heat equation\n    Args:\n        N: Size of the grid (N x N)\n\n    Returns:\n        Sparse 2D finite difference matrix A\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals[1] = diagonals[1].at[N-1::N].set(0)\n    diagonals[2] = diagonals[2].at[N-1::N].set(0)\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    return sparse.BCOO.fromdense(A)\n\n\n\nWe can see that the implementation is very similar to our implementation of get_A, however we use the sparse function on A in order to convert the matrix into a spare matrix.\nLet’s run another simulation but use the spare matrix this time to see if there is a decrease in the run time.\n\nimport heat_equation as eq\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nimport matplotlib.pyplot as plt\nimport time\n\n# Parameters\nN = 101\nepsilon = 0.2\niterations = 2700\nvisualization_interval = 300\n\n# Initialize the grid\nu = np.zeros((N, N))\nu[int(N/2), int(N/2)] = 1.0\n\n# Initialize list to store intermediate solutions\nintermediate_solutions = []\n\n# Get sparse matrix A\nA_sp_matrix = eq.get_sparse_A(N)\n\nstart_time_simulation = time.time()\n\n# Run simulation\nfor i in range(iterations):\n    # Advance the simulation by one timestep\n    u = eq.advance_time_matvecmul(A_sp_matrix, u, epsilon)\n    \n    # Store intermediate solutions every 300 iterations\n    if (i+1) % visualization_interval == 0:\n        intermediate_solutions.append(u)\n\nend_time_simulation = time.time()\nprint(f\"Total time taken for the simulation using JAX Sparse Matrix: {end_time_simulation - start_time_simulation} seconds\")\n\n# Visualize the diffusion of heat every 300 iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\n\nfor i in range(3):\n    for j in range(3):\n        index = i * 3 + j\n        iteration_number = (index + 1) * visualization_interval  # Calculate iteration number dynamically\n        axes[i, j].imshow(intermediate_solutions[index])\n        axes[i, j].set_title(f\"Iteration {iteration_number}\")\n\nplt.tight_layout()\nplt.show()\n\nTotal time taken for the simulation using JAX Sparse Matrix: 4.7250590324401855 seconds\n\n\n\n\n\nWe can see that this method of simulation gives the same output as before, but in much faster time. This method took about 4 seconds, which is about 34 times faster than the previous method.\nThis shows how powerful the sparse matrix is, as it was able to drastically cut down on the run time of the simulation.\nWe will hope to improve the effiency even further using our next method."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#direct-operation-with-numpy",
    "href": "posts/Heat Diffusion/index.html#direct-operation-with-numpy",
    "title": "Simulation of Heat Diffusion",
    "section": "Direct Operation with numpy",
    "text": "Direct Operation with numpy\nWe will use numpy’s np.roll() function in order to not have to do the full computation of the matrix multiplication. Using the roll() function in numpy we can instead find the value of the next matrix by using the surronding values of the matrix. This allows us to not have the full matrix mulitplication computation. Therefore, we hope that this new method would allow us to cut down on our run time.\nLet’s take a look at the advance_time_numpy function that uses the roll() function to cut down on computation time.\n\nfrom heat_equations import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the solution by one timestep using numpy operations\n    Args:\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    padded_u = np.pad(u, 1, mode='constant')  # Pad zeros to form (N+2) x (N+2) array\n    u_up = np.roll(padded_u, -1, axis=0)\n    u_down = np.roll(padded_u, 1, axis=0)\n    u_left = np.roll(padded_u, -1, axis=1)\n    u_right = np.roll(padded_u, 1, axis=1)\n    \n    u_new = u + epsilon * (u_up[1:-1, 1:-1] + u_down[1:-1, 1:-1] + u_left[1:-1, 1:-1] + u_right[1:-1, 1:-1] - 4 * u)\n    return u_new\n\n\n\nWe can see that we must pad the matrix with zeros using np.pad. We then shift around the padded array the first and second axis (rows and columns) in the left/right and up/down directions. This gives us four new matrices that we will use to calculate the next matrix, u_new, in the heat diffusion.\nLet’s run a script to see how this change performs.\n\nimport heat_equation as eq\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Parameters\nN = 101\nepsilon = 0.2\niterations = 2700\nvisualization_interval = 300\n\n# Initialize the grid\nu = np.zeros((N, N))\nu[int(N/2), int(N/2)] = 1.0\n\n# Initialize list to store intermediate solutions\nintermediate_solutions = []\n\n# Get matrix A\nA = eq.get_A(N)\n\n# Run simulation\nstart_time_simulation = time.time()\nfor i in range(iterations):\n    # Advance the simulation by one timestep\n    u = eq.advance_time_numpy(u, epsilon)\n    \n    # Store intermediate solutions every 300 iterations\n    if (i+1) % visualization_interval == 0:\n        intermediate_solutions.append(u)\n\nend_time_simulation = time.time()\nprint(f\"Total time taken for the simulation using direct operation: {end_time_simulation - start_time_simulation} seconds\")\n\n# Visualize the diffusion of heat every 300 iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\n\nfor i in range(3):\n    for j in range(3):\n        index = i * 3 + j\n        iteration_number = (index + 1) * visualization_interval  # Calculate iteration number dynamically\n        axes[i, j].imshow(intermediate_solutions[index])\n        axes[i, j].set_title(f\"Iteration {iteration_number}\")\n\nplt.tight_layout()\nplt.show()\n\nTotal time taken for the simulation using direct operation: 0.4622352123260498 seconds\n\n\n\n\n\nWe can see that we achieve the same output as before but the only change we made from the first script is replacing advance_time_matvecmul with advance_time_numpy.\nThis method of simulation only took about .46 seconds. This is about 10 times faster than method two and almost 300 times faster than the first method.\nThese results are very good and show how using direct operation instead of full matrix multiplication helps with effiency.\nWe will hope to improve on this with our last method."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#jax-just-in-time-compilation",
    "href": "posts/Heat Diffusion/index.html#jax-just-in-time-compilation",
    "title": "Simulation of Heat Diffusion",
    "section": "JAX Just-In-Time Compilation",
    "text": "JAX Just-In-Time Compilation\nWe will now try to decrease our run time using Just-In-Time (JIT) complilation from the JAX library. We will not use the sparse matrix from the second method, so the only difference would be to use the JIT advance time function instead of our original function.\nLet’s take a look at how we define advance_time_jax().\n\nfrom heat_equations import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the solution by one timestep using JAX operations\n    Args:\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    padded_u = jnp.pad(u, 1, mode='constant')  # Pad zeros to form (N+2) x (N+2) array\n    u_up = jnp.roll(padded_u, -1, axis=0)\n    u_down = jnp.roll(padded_u, 1, axis=0)\n    u_left = jnp.roll(padded_u, -1, axis=1)\n    u_right = jnp.roll(padded_u, 1, axis=1)\n    \n    u_new = u + epsilon * (u_up[1:-1, 1:-1] + u_down[1:-1, 1:-1] + u_left[1:-1, 1:-1] + u_right[1:-1, 1:-1] - 4 * u)\n    return u_new\n\n\n\nWe can see that this method is very similar to our previous method with numpy with a couple important exceptions. We include @jax.jit at the top of the function definition, this ensures that we use the Just-In-Time compilation from JAX. We also use the jax.numpy library as jnp.\nLet’s run a script to see how this method performs.\n\nimport heat_equations as eq\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Parameters\nN = 101\nepsilon = 0.2\niterations = 2700\nvisualization_interval = 300\n\n# Initialize the grid\nu = np.zeros((N, N))\nu[int(N/2), int(N/2)] = 1.0\n\n# Initialize list to store intermediate solutions\nintermediate_solutions = []\n\n# Run simulation\nstart_time_simulation = time.time()\nfor i in range(iterations):\n    # Advance the simulation by one timestep\n    u = eq.advance_time_jax(u, epsilon)\n    \n    # Store intermediate solutions every 300 iterations\n    if (i+1) % visualization_interval == 0:\n        intermediate_solutions.append(u)\n\nend_time_simulation = time.time()\nprint(f\"Total time taken for the simulation using Just-In-Time Compilation: {end_time_simulation - start_time_simulation} seconds\")\n\n# Visualize the diffusion of heat every 300 iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\n\nfor i in range(3):\n    for j in range(3):\n        index = i * 3 + j\n        iteration_number = (index + 1) * visualization_interval  # Calculate iteration number dynamically\n        axes[i, j].imshow(intermediate_solutions[index])\n        axes[i, j].set_title(f\"Iteration {iteration_number}\")\n\nplt.tight_layout()\nplt.show()\n\nTotal time taken for the simulation using Just-In-Time Compilation: 0.16453886032104492 seconds\n\n\n\n\n\nWe can see that the output is the same as before, however the method only took about .16 seconds. This is almost 3 times faster than our method from before using direct operations. Therefore by inlcuding JIT our effinency significantly increased. This is also over 800 times faster than our original method!"
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#run-time-comparison",
    "href": "posts/Heat Diffusion/index.html#run-time-comparison",
    "title": "Simulation of Heat Diffusion",
    "section": "Run Time Comparison",
    "text": "Run Time Comparison\nNow that we have tried all four of our different methods, we look at the run times of all the methods and compare them.\nLet’s run the following script to see how the run times compare.\n\n# Data\nmethods = ['Matrix Multiplication', 'Sparse Matrix', 'Direct Operation', 'Just-In-Time Compilation']\ntimes = [137.2656331062317, 4.7250590324401855, 0.4622352123260498, 0.16453886032104492]\n\n# Calculate speedup relative to Matrix Multiplication\nmatrix_mult_time = times[0]\nincrease = [matrix_mult_time / time for time in times]\n\n# Display method names, run times, and speedups\nfor method, time, speedup in zip(methods, times, speedups):\n    print(f\"{method}: {time:.2f} seconds (Increase: {speedup:.2f}x)\")\n\nMatrix Multiplication: 137.27 seconds (Increase: 1.00x)\nSparse Matrix: 4.73 seconds (Increase: 29.05x)\nDirect Operation: 0.46 seconds (Increase: 296.96x)\nJust-In-Time Compilation: 0.16 seconds (Increase: 834.24x)\n\n\nWe an see that we have displayed the run times of each method along with how much faster the method is compared to the Matrix Multiplication method.\nWe see that Direct Operation and JIT Compilation were very fast, while Matrix Multiplication took a very long time. Therefore we should try to stick to Direct Operation and JIT Compilation when doing simulations that require a lot of calculations."
  },
  {
    "objectID": "posts/Heat Diffusion/index.html#outro",
    "href": "posts/Heat Diffusion/index.html#outro",
    "title": "Simulation of Heat Diffusion",
    "section": "Outro",
    "text": "Outro\nThank you for reading my blog post! I hope you were able to learn more about simulations and run times in regard to the four different methods that we covered!"
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html",
    "href": "posts/Country Temps Visualization/index.html",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Hello! Welcome to this blog post where I will go over how to use databases for data visualization. For this post we will be using example data of temperature data from various different countries.\n\n\nFirst we need to run the following line of code:\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nFor this post, we are going to be using SQL queries in order to retrevie our data. Then we are going to use some data visualization software to answer a few questions. We will be using sqlite to establish a database and to retreive our data.\n\n\n\nIn a seperate file, I already loaded in the data into a database named “database.db”. Thus we will connect to this database in order to get information about our data. The data we are using consists of three different tables: stations, countries, and temperatures. This data has been collected and shows the temperature readings of different stations in various countries over the years.\nWe will use this data to answer our first question: How does the average yearly change in temperature vary within a given country?\nThus we need to use an SQL query to acess the data that is useful for answering this question.\nWe will also need to include the headers necessary for the rest of the post.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nHere is the query we will use. The function allows a user a to input a database file, country, year_begin, year_end, and month to find the data needed to answer our question.\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query the climate database to retrieve temperature data for a specific country within a given time frame and month.\n\n    Parameters:\n        db_file : The path to the SQLite database file.\n        country : The name of the country to retrieve data for.\n        year_begin : The starting year of the time frame.\n        year_end : The ending year of the time frame.\n        month : The month to retrieve data for.\n        min_obs : The minimum number of years of observation required for a station to be included in the analysis.\n\n    Returns:\n        DataFrame: A DataFrame containing temperature data for the specified country and time frame.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n            SELECT\n                stations.NAME,\n                stations.LATITUDE,\n                stations.LONGITUDE,\n                countries.Name AS Country,\n                temperatures.Year,\n                temperatures.Month,\n                temperatures.Temp\n            FROM\n                temperatures\n            JOIN\n                stations ON temperatures.ID = stations.ID\n            JOIN\n                countries ON SUBSTR(stations.ID, 1, 2) = SUBSTR(countries.\"FIPS 10-4\", 1, 2)\n            WHERE\n                countries.Name = '{country}'\n                AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n                AND temperatures.Month = {month}\n        \"\"\"\n\n        # Execute the query and read the results into a DataFrame\n        result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return result_df\n\nNow in our dataframe we have the data that we will use the answer our question: the station’s name, latitude, and longitude, the country’s name, and the tempature of the country and the month we are looking at. This data is stored in result_df.\nThus, we can now write a fucntion, temperature_coefficient_plot to use the pulled data and make a plot. The goal of this function is to create a plot of the selected country with a scatter of data points located at the stations used. Each datapoint will store information about the average yearly change in temperature for the associated station.\nThe function uses the same variables as the previous function, in addition to mins_obs, zoom, mapbox_style, and color_continuous_scale. mins_obs is an inputed variable that only shows stations that have at least mins_obs years of observatoin. The other variables, zoom, mapbox_style, and color_continuous_scale, are for graphing preference of the plot.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, zoom =2, mapbox_style=\"carto-positron\", color_continuous_scale=px.colors.diverging.RdGy_r ):\n    \"\"\"\n    Create a plot showing the expected temperature change across different stations within a given country.\n\n    Parameters:\n        db_file : The path to the SQLite database file.\n        country : The name of the country to visualize.\n        year_begin : The starting year of the time frame.\n        year_end : The ending year of the time frame.\n        month : The month to analyze.\n        min_obs : The minimum number of years of observation required for a station to be included in the analysis.\n        zoom : The initial zoom level for the map plot. Default is 2.\n        mapbox_style : The map style to use. Default is \"carto-positron\".\n        color_continuous_scale : The color scale for the scatter plot. Default is 'RdGy_r'.\n\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    grouped_df = df.groupby(\"NAME\")\n\n    # Create a dictionary to hold each station and their info\n\n    dfs_dict = {name: group for name, group in grouped_df}\n\n    # Make sure to only include stations that meet the criteria that we need\n    filtered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n    # Calculate the expected change\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    results = pd.DataFrame()\n    \n    # Add stations along with expected change into the results data frame\n    for name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\n    # Create scatterplot to plot the data points\n    fig = px.scatter_mapbox(\n    results,\n    lat=\"LATITUDE\",\n    lon=\"LONGITUDE\",\n    text=\"NAME\",\n    color=\"EXPECTED_CHANGE\",\n    color_continuous_scale=color_continuous_scale,\n    size_max=30,\n    zoom=zoom,\n    mapbox_style=mapbox_style,\n    title=f\"Expected Temperature Change in {country} ({year_begin}-{year_end}), Month {month}\",\n    )\n\n    fig.show()\n\nThe function first pulls the df using the query_climate_database function, then groups the results by station name.\nThis is what the data frame would look like after being running the query_calimate_database function. As we can see the result is a 3152 x 7 dataframe that contains the desired information.\n\n#Example data\n(db_file, country, year_begin, year_end, month, min_obs) = (\"database.db\", \"India\", 1980, 2020, 1, 10)\n\ndf = query_climate_database(db_file, country, year_begin, year_end, month)\n\nprint(df)\n\ngrouped_df = df.groupby(\"NAME\")\n\n               NAME  LATITUDE  LONGITUDE Country  Year  Month   Temp\n0     PBO_ANANTAPUR    14.583     77.633   India  1980      1  23.48\n1     PBO_ANANTAPUR    14.583     77.633   India  1981      1  24.57\n2     PBO_ANANTAPUR    14.583     77.633   India  1982      1  24.19\n3     PBO_ANANTAPUR    14.583     77.633   India  1983      1  23.51\n4     PBO_ANANTAPUR    14.583     77.633   India  1984      1  24.81\n...             ...       ...        ...     ...   ...    ...    ...\n3147     DARJEELING    27.050     88.270   India  1983      1   5.10\n3148     DARJEELING    27.050     88.270   India  1986      1   6.90\n3149     DARJEELING    27.050     88.270   India  1994      1   8.10\n3150     DARJEELING    27.050     88.270   India  1995      1   5.60\n3151     DARJEELING    27.050     88.270   India  1997      1   5.70\n\n[3152 rows x 7 columns]\n\n\nNext, the function creates a seperate dataframe for each station that contains all that stations data. It also filters out any statoins with less than mins_obs years of data. It will store these dataframes in a dictionary where the key will be the name of the station.\nNow that we have the dictionary of station’s data, we use linear regression from the scikit.learn libray, to fit a line of best fit for each station. Thus, the slope of this line will be the rate of change for the temperature at the given station. We will put each station and its rate of change in a seperate dataframe along with the station longitude and latitude. This information will be stored in a dataframe called results.\nSo the list of the rates of change will look like the following.\n\n# Create dictionary for dataframes, each keyed by the unique \"NAME\"\ndfs_dict = {name: group for name, group in grouped_df}\n\nfiltered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n# Calculate expected change in temp for each station\ndef coef(data_group):\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nresults = pd.DataFrame()\n\n# Iterate through the dictionary and add the name, lat, long, and change to a list of results\nfor name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\nresults\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nEXPECTED_CHANGE\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\n-0.006184\n\n\n1\nAHMADABAD\n23.067\n72.633\n0.006731\n\n\n2\nAKOLA\n20.700\n77.033\n-0.008063\n\n\n3\nALLAHABAD\n25.441\n81.735\n-0.029375\n\n\n4\nALLAHABAD_BAMHRAULI\n25.500\n81.900\n-0.015457\n\n\n...\n...\n...\n...\n...\n\n\n92\nTRIVANDRUM\n8.500\n77.000\n0.022892\n\n\n93\nUDAIPUR_DABOK\n24.617\n73.883\n0.072424\n\n\n94\nVARANASI_BABATPUR\n25.450\n82.867\n-0.012996\n\n\n95\nVERAVAL\n20.900\n70.367\n0.024848\n\n\n96\nVISHAKHAPATNAM\n17.717\n83.233\n-0.034050\n\n\n\n\n97 rows × 4 columns\n\n\n\nFinally, using plotly, we will graph this data on the plot of the chosen country. The function temperature_coefficient_plot has the variables, zoom, mapbox_style, and color_continuous_scale, predefined. Thus a user can choose to input their own prefrences for these variables but if not it will be set to the setting chosen in the function declaration.\nLet’s use our new function to look a plot of an example country. For example lets look at the country of India from years 1980-2020, during the month of January. For this example, we will use a min_obs of 10 years and use the default settings for the plotly graph.\n\n# Example variables\n(country, year_begin, year_end, month, min_obs) = (\"India\", 1980, 2020, 1, 10)\n\n# Call function\ntemperature_coefficient_plot(\"database.db\", country, year_begin, year_end, month, min_obs)\n\n\n\n\nAs we can see an interactive graph was produced by our function. We can zoom in and out on the graph and we are also able to hover over the datapoints to see what the expected rate of change of temperature will be. We can also see that there is a scale on the right that shows how the color of the data point correlates to the rate of change.\nWe can also change to the variables of the function to examine another country and time period. For example, if we wanted to choose Italy from 1990-2010 in March with at least 15 oberservations, we would have the following function call and output.\n\n# Second example variables\n(country, year_begin, year_end, month, min_obs) = (\"Italy\", 1990, 2010, 3, 15)\n\n# Call function\ntemperature_coefficient_plot(\"database.db\", country, year_begin, year_end, month, min_obs)\n\n\n\n\nWe can see that another interactive graph is displayed and the scatterplots are overlayed on the map of Italy. Something intersting to note in this example is that they are several more observation points in central and north Italy, compared to southern parts of the country. This can be due several reasons, such as not having as much stations in the south or not having as many stations that meet the criteria of at least 15 observations."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#set-up",
    "href": "posts/Country Temps Visualization/index.html#set-up",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "First we need to run the following line of code:\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nFor this post, we are going to be using SQL queries in order to retrevie our data. Then we are going to use some data visualization software to answer a few questions. We will be using sqlite to establish a database and to retreive our data."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#first-question-and-query",
    "href": "posts/Country Temps Visualization/index.html#first-question-and-query",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "In a seperate file, I already loaded in the data into a database named “database.db”. Thus we will connect to this database in order to get information about our data. The data we are using consists of three different tables: stations, countries, and temperatures. This data has been collected and shows the temperature readings of different stations in various countries over the years.\nWe will use this data to answer our first question: How does the average yearly change in temperature vary within a given country?\nThus we need to use an SQL query to acess the data that is useful for answering this question.\nWe will also need to include the headers necessary for the rest of the post.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nHere is the query we will use. The function allows a user a to input a database file, country, year_begin, year_end, and month to find the data needed to answer our question.\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query the climate database to retrieve temperature data for a specific country within a given time frame and month.\n\n    Parameters:\n        db_file : The path to the SQLite database file.\n        country : The name of the country to retrieve data for.\n        year_begin : The starting year of the time frame.\n        year_end : The ending year of the time frame.\n        month : The month to retrieve data for.\n        min_obs : The minimum number of years of observation required for a station to be included in the analysis.\n\n    Returns:\n        DataFrame: A DataFrame containing temperature data for the specified country and time frame.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n            SELECT\n                stations.NAME,\n                stations.LATITUDE,\n                stations.LONGITUDE,\n                countries.Name AS Country,\n                temperatures.Year,\n                temperatures.Month,\n                temperatures.Temp\n            FROM\n                temperatures\n            JOIN\n                stations ON temperatures.ID = stations.ID\n            JOIN\n                countries ON SUBSTR(stations.ID, 1, 2) = SUBSTR(countries.\"FIPS 10-4\", 1, 2)\n            WHERE\n                countries.Name = '{country}'\n                AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n                AND temperatures.Month = {month}\n        \"\"\"\n\n        # Execute the query and read the results into a DataFrame\n        result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return result_df\n\nNow in our dataframe we have the data that we will use the answer our question: the station’s name, latitude, and longitude, the country’s name, and the tempature of the country and the month we are looking at. This data is stored in result_df.\nThus, we can now write a fucntion, temperature_coefficient_plot to use the pulled data and make a plot. The goal of this function is to create a plot of the selected country with a scatter of data points located at the stations used. Each datapoint will store information about the average yearly change in temperature for the associated station.\nThe function uses the same variables as the previous function, in addition to mins_obs, zoom, mapbox_style, and color_continuous_scale. mins_obs is an inputed variable that only shows stations that have at least mins_obs years of observatoin. The other variables, zoom, mapbox_style, and color_continuous_scale, are for graphing preference of the plot.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, zoom =2, mapbox_style=\"carto-positron\", color_continuous_scale=px.colors.diverging.RdGy_r ):\n    \"\"\"\n    Create a plot showing the expected temperature change across different stations within a given country.\n\n    Parameters:\n        db_file : The path to the SQLite database file.\n        country : The name of the country to visualize.\n        year_begin : The starting year of the time frame.\n        year_end : The ending year of the time frame.\n        month : The month to analyze.\n        min_obs : The minimum number of years of observation required for a station to be included in the analysis.\n        zoom : The initial zoom level for the map plot. Default is 2.\n        mapbox_style : The map style to use. Default is \"carto-positron\".\n        color_continuous_scale : The color scale for the scatter plot. Default is 'RdGy_r'.\n\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    grouped_df = df.groupby(\"NAME\")\n\n    # Create a dictionary to hold each station and their info\n\n    dfs_dict = {name: group for name, group in grouped_df}\n\n    # Make sure to only include stations that meet the criteria that we need\n    filtered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n    # Calculate the expected change\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    results = pd.DataFrame()\n    \n    # Add stations along with expected change into the results data frame\n    for name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\n    # Create scatterplot to plot the data points\n    fig = px.scatter_mapbox(\n    results,\n    lat=\"LATITUDE\",\n    lon=\"LONGITUDE\",\n    text=\"NAME\",\n    color=\"EXPECTED_CHANGE\",\n    color_continuous_scale=color_continuous_scale,\n    size_max=30,\n    zoom=zoom,\n    mapbox_style=mapbox_style,\n    title=f\"Expected Temperature Change in {country} ({year_begin}-{year_end}), Month {month}\",\n    )\n\n    fig.show()\n\nThe function first pulls the df using the query_climate_database function, then groups the results by station name.\nThis is what the data frame would look like after being running the query_calimate_database function. As we can see the result is a 3152 x 7 dataframe that contains the desired information.\n\n#Example data\n(db_file, country, year_begin, year_end, month, min_obs) = (\"database.db\", \"India\", 1980, 2020, 1, 10)\n\ndf = query_climate_database(db_file, country, year_begin, year_end, month)\n\nprint(df)\n\ngrouped_df = df.groupby(\"NAME\")\n\n               NAME  LATITUDE  LONGITUDE Country  Year  Month   Temp\n0     PBO_ANANTAPUR    14.583     77.633   India  1980      1  23.48\n1     PBO_ANANTAPUR    14.583     77.633   India  1981      1  24.57\n2     PBO_ANANTAPUR    14.583     77.633   India  1982      1  24.19\n3     PBO_ANANTAPUR    14.583     77.633   India  1983      1  23.51\n4     PBO_ANANTAPUR    14.583     77.633   India  1984      1  24.81\n...             ...       ...        ...     ...   ...    ...    ...\n3147     DARJEELING    27.050     88.270   India  1983      1   5.10\n3148     DARJEELING    27.050     88.270   India  1986      1   6.90\n3149     DARJEELING    27.050     88.270   India  1994      1   8.10\n3150     DARJEELING    27.050     88.270   India  1995      1   5.60\n3151     DARJEELING    27.050     88.270   India  1997      1   5.70\n\n[3152 rows x 7 columns]\n\n\nNext, the function creates a seperate dataframe for each station that contains all that stations data. It also filters out any statoins with less than mins_obs years of data. It will store these dataframes in a dictionary where the key will be the name of the station.\nNow that we have the dictionary of station’s data, we use linear regression from the scikit.learn libray, to fit a line of best fit for each station. Thus, the slope of this line will be the rate of change for the temperature at the given station. We will put each station and its rate of change in a seperate dataframe along with the station longitude and latitude. This information will be stored in a dataframe called results.\nSo the list of the rates of change will look like the following.\n\n# Create dictionary for dataframes, each keyed by the unique \"NAME\"\ndfs_dict = {name: group for name, group in grouped_df}\n\nfiltered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n# Calculate expected change in temp for each station\ndef coef(data_group):\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nresults = pd.DataFrame()\n\n# Iterate through the dictionary and add the name, lat, long, and change to a list of results\nfor name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\nresults\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nEXPECTED_CHANGE\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\n-0.006184\n\n\n1\nAHMADABAD\n23.067\n72.633\n0.006731\n\n\n2\nAKOLA\n20.700\n77.033\n-0.008063\n\n\n3\nALLAHABAD\n25.441\n81.735\n-0.029375\n\n\n4\nALLAHABAD_BAMHRAULI\n25.500\n81.900\n-0.015457\n\n\n...\n...\n...\n...\n...\n\n\n92\nTRIVANDRUM\n8.500\n77.000\n0.022892\n\n\n93\nUDAIPUR_DABOK\n24.617\n73.883\n0.072424\n\n\n94\nVARANASI_BABATPUR\n25.450\n82.867\n-0.012996\n\n\n95\nVERAVAL\n20.900\n70.367\n0.024848\n\n\n96\nVISHAKHAPATNAM\n17.717\n83.233\n-0.034050\n\n\n\n\n97 rows × 4 columns\n\n\n\nFinally, using plotly, we will graph this data on the plot of the chosen country. The function temperature_coefficient_plot has the variables, zoom, mapbox_style, and color_continuous_scale, predefined. Thus a user can choose to input their own prefrences for these variables but if not it will be set to the setting chosen in the function declaration.\nLet’s use our new function to look a plot of an example country. For example lets look at the country of India from years 1980-2020, during the month of January. For this example, we will use a min_obs of 10 years and use the default settings for the plotly graph.\n\n# Example variables\n(country, year_begin, year_end, month, min_obs) = (\"India\", 1980, 2020, 1, 10)\n\n# Call function\ntemperature_coefficient_plot(\"database.db\", country, year_begin, year_end, month, min_obs)\n\n\n\n\nAs we can see an interactive graph was produced by our function. We can zoom in and out on the graph and we are also able to hover over the datapoints to see what the expected rate of change of temperature will be. We can also see that there is a scale on the right that shows how the color of the data point correlates to the rate of change.\nWe can also change to the variables of the function to examine another country and time period. For example, if we wanted to choose Italy from 1990-2010 in March with at least 15 oberservations, we would have the following function call and output.\n\n# Second example variables\n(country, year_begin, year_end, month, min_obs) = (\"Italy\", 1990, 2010, 3, 15)\n\n# Call function\ntemperature_coefficient_plot(\"database.db\", country, year_begin, year_end, month, min_obs)\n\n\n\n\nWe can see that another interactive graph is displayed and the scatterplots are overlayed on the map of Italy. Something intersting to note in this example is that they are several more observation points in central and north Italy, compared to southern parts of the country. This can be due several reasons, such as not having as much stations in the south or not having as many stations that meet the criteria of at least 15 observations."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#question-3-how-does-the-temperature-vary-across-different-latitudes-within-a-list-of-countries-for-a-given-month",
    "href": "posts/Country Temps Visualization/index.html#question-3-how-does-the-temperature-vary-across-different-latitudes-within-a-list-of-countries-for-a-given-month",
    "title": "Data Visualization and Databases",
    "section": "Question 3: How does the temperature vary across different latitudes within a list of countries for a given month?",
    "text": "Question 3: How does the temperature vary across different latitudes within a list of countries for a given month?\nThis next question we try to answer is intruiging as we can see if there is a correlation between latitude and temperature for a list of countries within a given month.\nWe first must define our new query function, query_temperature_by_latitude, which requires a user to input a county and a month to be visualized.\nThe query returns the name and latitude of the station as well as the average temperature for the selected station.\n\ndef query_temperature_by_latitude(db_file, country, month):\n    \"\"\"\n    Query the climate database to retrieve temperature data for a specific country and month, grouped by latitude.\n\n    Parameters:\n        db_file : The path to the SQLite database file.\n        country : The name of the country to retrieve data for.\n        month : The month to retrieve data for.\n\n    Returns:\n        DataFrame: A DataFrame containing temperature data grouped by latitude for the specified country and month.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n            SELECT\n                stations.NAME,\n                stations.LATITUDE,\n                AVG(temperatures.Temp) AS AvgTemperature\n            FROM\n                temperatures\n            JOIN\n                stations ON temperatures.ID = stations.ID\n            JOIN\n                countries ON SUBSTR(stations.ID, 1, 2) = SUBSTR(countries.\"FIPS 10-4\", 1, 2)\n            WHERE\n                countries.Name = '{country}'\n                AND temperatures.Month = {month}\n            GROUP BY\n                stations.NAME, stations.LATITUDE\n            ORDER BY\n                stations.LATITUDE\n        \"\"\"\n\n        result_df = pd.read_sql_query(cmd, conn)\n\n    return result_df\n\nLet’s take a look at what this resulting data frame will look like, using the United States and the month of March.\n\ndf = query_temperature_by_latitude(db_file, \"United States\", 3)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nAvgTemperature\n\n\n\n\n0\nNAALEHU_14\n19.0614\n21.607500\n\n\n1\nKIOLAKAA_7\n19.0667\n20.521212\n\n\n2\nSOUTH_KONA_2232\n19.0825\n18.292609\n\n\n3\nSEA_MTN_1215\n19.1336\n22.496250\n\n\n4\nPAHALA_21\n19.1986\n19.954773\n\n\n...\n...\n...\n...\n\n\n12548\nALPINE\n70.3464\n-22.067143\n\n\n12549\nCOLVILLE_VILLAGE\n70.4322\n-25.835909\n\n\n12550\nWAINWRIGHT_AP\n70.6392\n-24.802381\n\n\n12551\nBARROW_POST_ROGERS_AP\n71.2833\n-25.335980\n\n\n12552\nBARROW_4_ENE\n71.3214\n-23.970000\n\n\n\n\n12553 rows × 3 columns\n\n\n\nNext, we define our function plot_temperature_by_latitude, that will make use of multiple facets to give us the scatter plot of all the stations used for a given list of countries called selected_countries. The default values for selected countries is United States, Canada, and Mexico.\nThe function again uses the plotly library to produce the graph.\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport plotly.subplots as ps\n\ndef plot_temperature_by_latitude(db_file, countries, month):\n    \"\"\"\n    Create a plot showing the variation of temperature across different latitudes within a list of countries for a given month.\n\n    Parameters:\n        db_file : The path to the SQLite database file.\n        countries : A list of countries to include in the plot.\n        month : The month to analyze.\n    \"\"\"\n    fig = ps.make_subplots(rows=len(countries), cols=1, subplot_titles=countries)  # Create subplots for each country\n    \n    # Iterate over each country and create a scatter plot to show temp at each station based on latitude\n    for i, country in enumerate(countries, start=1):\n        result_df = query_temperature_by_latitude(db_file, country, month)\n\n        scatter = go.Scatter(\n            x=result_df[\"LATITUDE\"],\n            y=result_df[\"AvgTemperature\"],\n            mode='markers',\n            name=f\"{country}\",\n            text=result_df[\"NAME\"],\n            marker=dict(\n                size=10,\n                color=result_df[\"AvgTemperature\"],\n                colorscale='Viridis',\n                line=dict(width=0.5, color='DarkSlateGrey')\n            ),\n            showlegend=False  # Do not show legend for individual plots\n        )\n        \n        fig.add_trace(scatter, row=i, col=1)  # Add trace to corresponding subplot\n\n    fig.update_layout(\n        title=f\"Temperature Variation by Latitude - Month {month}\",\n        xaxis=dict(title='Latitude'),\n        yaxis=dict(title='Average Temperature'),\n        hovermode='closest',\n        height=800,  # Adjust the height of the entire figure\n    )\n\n    pio.show(fig)\n\nNow that we have defined our plotting function, let’s plot the temperatures in the United States, Canada, and Mexico and see if we can find any interesting trends.\n\nplot_temperature_by_latitude(db_file, [\"United States\", \"Canada\", \"Mexico\"], 3)\n\n\n\n\nWe can see that the output is a visualization with multiple facets. Three plots are produced, one for each of the listed countries. The plots show the latidude of the station and the average temperature at each station.\nAs we can see in the plot, there is a clear trend in the given countries. As latitude inrcreaes, the average temperature decreases. This intuitively makes sense. For example in the plot for the United States, we see that the stations in Hawaii are the warmest, while the stations in Alaska are the coldest. This placement is correct as we know that Hawaii is one of the hotest states and Alaska one of the coldest.\nThe graph is also interactive as the user is able to hover over each scatterpoint on the graph and check its latitude and average temperature. There is also a color coded scale on the right that shows the warmer stations as warm colors (yellow, orange) and the colder staions as cold colors (purple, blue)."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#conclusion",
    "href": "posts/Country Temps Visualization/index.html#conclusion",
    "title": "Data Visualization and Databases",
    "section": "Conclusion",
    "text": "Conclusion\nIn conlusion, we can see how helpful SQL and graphing software like plotly can be for data visualization. I hope that you enjoyed reading my blog post and learned something interesting!"
  },
  {
    "objectID": "posts/Flask Web Dev/index.html",
    "href": "posts/Flask Web Dev/index.html",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Hello! Welcome to this blog post where we will be going over how to create a webstie using Flask.\n\n\nOur goal is to create a website using Flask, a python package, thats main purpose is to be a message board. We want the website to have the functionaility of having users being able to post messages to the messages board. When the users want to post a message, they should be prompted with two fields: one for their name and one for their message.\nNext, we want a user to be able to view messages of the message board as well. When the user wants to view messages they should see a defined number of random messages from the message board.\nFinally, we want to add some style to our website using CSS so the website is not boring and can be more engaging.\n\n\n\nFirst we need to include our neccesary packages into our main file, called app.py.\nWill also create our app, named app, that uses flask.\n\nimport sqlite3\nimport pandas as pd\n\nfrom flask import Flask, g, render_template, request\n\napp = Flask(__name__)\n\n\n\n\nNext, using sqlite, we will create and host a database. This database will contain all the messages that different users submit to our message board. This database will also be used later on to display some of the messages when the user choses to view messages.\nHere is how the function, get_message_db(), is defined:\n\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        \n        # Create the messages table if it doesn't exist\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (name TEXT, message TEXT);'\n        cursor.execute(cmd)\n        g.message_db.commit()\n        \n        return g.message_db\n\nWe can see that at first we try to access the database. If it not is created then we get an attribuite error so we will connect to sqlite and create our database.\nWe then create a cursor using the .cursor() function that will allow us to use SQL code to create our database.\nWe then use CREATE TABLE to make our table called messaages. This table has two columns, name and message, as these are the only things that user is able to submit to the website.\nFinally, we execute and commit the SQL code and then return the database.\n\n\n\nWe wil next go over how to insert user messages into the database that we just created. We will do this by using the insert_messages() function. This function takes in one variable, request. From request we can access the name and the message that we want to insert into the database. We will be going how we got the request later on when we go over our routes.\nHere is the defintion for our insert_messages() function:\n\ndef insert_message(request):\n    # Get the message and the name\n    message = request.form['message']\n    name = request.form[\"name\"]\n    db = get_message_db()\n    cursor = db.cursor()\n    print(\"here\")\n    # Insert the data into our database\n    cursor.execute('INSERT INTO messages (name, message) VALUES (?, ?)', (name, message))\n    db.commit()\n    cursor.close()\n    db.close()\n    return message, name\n\nIn the function we first extract the message and name from the request and store them under the same variable names.\nNext, use our previous function to get the database and store as the variable db.\nThen we use the cursur to write an SQL command to insert the message and name as a tuple into our database.\nMake sure to close the cursor and databse, and then we return the tuple as well.\n\n\n\nOur last main function we need to implement is the random messages function. The goal of this function is to retrieve an n amount of random messages from our database. We will later use this function to display these messages on the website.\nThe function is defined as the following:\n\ndef random_messages(n):\n    # Connect to database\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # Find n random messages\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    random_messages = cursor.fetchall()\n\n    # Close database\n    cursor.close()\n    db.close()\n\n    return random_messages\n\nWe can see we use the same idea as before, connecting to the database with the cursor. The main part of this function is using the ORDER BY and RANDOM() SQL commands to grab the first n tuples in the database. Since we are using RANDOM() then these n tuples will be different every time and ensures randomness.\nFinally, we close the database and return the random messages.\n\n\n\nNow that we have finished writing our main three functions, we can write our routes for the website. This is the backend implementation of what is happening when the user goes onto the post message or view message page, as well as when the user decides to post a message.\n\n\n\n\n@app.route('/')\ndef home():\n    return render_template('base.html')\n\nWe can see that our first route is very simple. This is for when we first get onto the website, so the route is just a slash which leads to a base page. When we get onto this page we render the base.html file, which contains two options for the user: post message or view message.\nWe will go over the set up of base.html and the other templates after we finish going over the routes.\n\n\n\nNext, we will write the route for the submit page, which will be /submit/. This route should allow the user to post a message onto the message board. This message should then be entered into the database.\nHere is the implementaion of the submit route:\n\n# Route to render the submit page\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        insert_message(request)\n        return render_template('submit.html')\n\nWe can see that within the route that we have two methods: POST and GET.\nIf the method is GET then we simply render the submit.html page. However if the method is POST, then the user wants to post a message onto the board. Therefore we use the insert_message function from earlier to insert the data into the databse. We still render the submit.html page in this method as well.\n\n\n\nOur final route is the view route. This is for when the user wants view some of the random messages in the databse. We will access this route using /view.\nHere is the implementation of the view route:\n\n@app.route('/view')\ndef view():\n    try:\n        messages = random_messages(5)\n        return render_template('view.html', messages=messages)\n    except:\n        return render_template\n\nWe can see that we use the random_messages function with n = 5 and store the messages in the messages variable. For now we are just using n = 5 but we can always change this later. We then render the view.html page along with the messages.\n\n\n\nNow that we have gone over functions and routes of our website, we will go over our main three templates.\n\n\n\nOur first template is names base.html. This serves as the landing page for our website, so when we enter the address the site this is where we are taken. The base.html site will greet the user and ask them to either post a message onto the board or view some random messages.\nWe will also use the base.html to build our other two templates: submit and view.\nHere is the set up of base.html:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{% block title %}{% endblock %} Message Bank&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;nav&gt;\n    &lt;h1&gt;Welcome to the Message Bank!&lt;/h1&gt;\n    &lt;p&gt;Hi! Welcome to my message bank. Please feel free to post a new message by clicking on the &lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt; link. You can also view some random messages from the bank by clicking &lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;. Hope you enjoy!&lt;/p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/nav&gt;    \n\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\nWe can see that the some of this file is styling the page. This was actually really fun as I got to mess around with different styles and colors.\nHowever the most important part of this file is the following section:\n\n&lt;nav&gt;\n    &lt;h1&gt;Welcome to the Message Bank!&lt;/h1&gt;\n    &lt;p&gt;Hi! Welcome to my message bank. Please feel free to post a new message by clicking on the &lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt; link. You can also view some random messages from the bank by clicking &lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;. Hope you enjoy!&lt;/p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/nav&gt; \n\nThis section greats the user by welcoming them to the message bank and telling the user the options they can do from here.\nWe also create two links for the user “Post Messages” and “View Messages” that would lead to the submit and view routes respectively.\nTherefore once a user chooses what they want to do they are redirected to the correct page.\nWe can also view the style.css file that I used for styling the base page:\n\nbody {\n    background-color: #192841; /* Dark blue background */\n    font-family: Arial, sans-serif;\n}\nnav {\n    border-bottom: 2px solid gold;\n    padding-bottom: 20px;\n    margin-bottom: 20px;\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);\n}\nh1 {\n    color: white;\n    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\n    font-size: 28px;\n}\np {\n    color: orange;\n    font-size: 18px;\n    line-height: 1.5;\n    margin-bottom: 20px;\n}\np a {\n    color: gold;\n    text-decoration: none;\n    transition: color 0.3s ease;\n}\np a:hover {\n    color: lightgoldenrodyellow;\n}\nul {\n    list-style-type: none;\n    padding: 0;\n    margin: 0;\n}\nul li {\n    display: inline-block;\n    margin-right: 20px;\n}\nul li a {\n    color: gold;\n    font-size: 20px;\n    text-decoration: none;\n    border-bottom: 2px solid transparent;\n    transition: border-bottom-color 0.3s ease;\n}\nul li a:hover {\n    border-bottom-color: gold;\n}\n\n\n\n\nNext, we have a template for the submit page called submit.html. Here the user is able to submit a message to the message bank.\nHere is the template for submit.html:\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1 style=\"color: green; font-family: 'Roboto', sans-serif; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5)\"&gt;{% block title %}&lt;span class=\"protest-guerrilla-regular\"&gt;Submit a Message&lt;/span&gt;{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n\n\n{% block content %}\n\n  &lt;style&gt;\n    /* Import Google Fonts */\n    @import url('https://fonts.googleapis.com/css2?family=Protest+Guerrilla&display=swap');\n\n    /* Apply custom font */\n    .protest-guerrilla-regular {\n      font-family: \"Protest Guerrilla\", sans-serif;\n      font-weight: 400;\n      font-style: normal;\n    }\n\n    /* Set background color */\n    body {\n      background-color:  #192841;\n    }\n  &lt;/style&gt;\n\n  &lt;form method=\"post\"&gt;\n      &lt;label for=\"name\" class=\"protest-guerrilla-regular\"&gt;What's your name?&lt;/label&gt;\n      &lt;input name=\"name\" id=\"name\" class=\"protest-guerrilla-regular\"&gt;\n      &lt;label for=\"message\" class=\"protest-guerrilla-regular\"&gt;What is your message?&lt;/label&gt;\n      &lt;input type=\"text\" name=\"message\" id=\"message\" class=\"protest-guerrilla-regular\"&gt;\n      &lt;input type=\"submit\" value=\"Post Message\" class=\"protest-guerrilla-regular\"&gt;\n  &lt;/form&gt;\n  {% if sec == True %}\n  &lt;br&gt;\n      &lt;b&gt;Message sent&lt;/b&gt;\n  {% endif %}\n{% endblock %}\n\nAgain, some of this file is the style prefernces for the page. I choose to use a google api to get the Protest Guerrilla font for some of this page.\nWe can examine the most important part of the file below:\n\n&lt;form method=\"post\"&gt;\n    &lt;label for=\"name\" class=\"protest-guerrilla-regular\"&gt;What's your name?&lt;/label&gt;\n    &lt;input name=\"name\" id=\"name\" class=\"protest-guerrilla-regular\"&gt;\n    &lt;label for=\"message\" class=\"protest-guerrilla-regular\"&gt;What is your message?&lt;/label&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\" class=\"protest-guerrilla-regular\"&gt;\n    &lt;input type=\"submit\" value=\"Post Message\" class=\"protest-guerrilla-regular\"&gt;\n&lt;/form&gt;\n\nWe can see that there are two prompts for the user: “What’s your name?” and “What is your message?”. We can see that these are then called name and message respectively. So once the user submits their info the input_message function will be called and their info will be entered into the database.\nThe html page is also based on base.html as we include {% extends ‘base.html’ %} at the top of the page. Therefore the writing of the home page is still there and the prompts show up below.\n\n\n\nFinally, we have our third template, view.html. This page will allow the user to view the random messages from the message bank.\nHere is the implementation of the view.html template:\n\n{% extends 'base.html' %}\n\n{% block title %}\n    Random Messages\n{% endblock %}\n\n{% block content %}\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Random Messages&lt;/h1&gt;\n        &lt;ul&gt;\n            {% for message in messages %}\n                &lt;li&gt;\n                    &lt;div class=\"message\"&gt;\n                        &lt;p&gt;{{ message[0] }}&lt;/p&gt; &lt;!-- name --&gt;\n                        &lt;p&gt;{{ message[1] }}&lt;/p&gt; &lt;!-- message --&gt;\n                    &lt;/div&gt;\n                &lt;/li&gt;\n            {% endfor %}\n        &lt;/ul&gt;\n    &lt;/div&gt;\n{% endblock %}\n\n{% block header %}\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='view_style.css') }}\"&gt;\n{% endblock %}\n\nAs before, we still use base.html to extend this template and some the code is for styling. As I wanted this page to have its own style for the viewing of the message bank, I created a new file called view_style.css for the style of this page. We can take a look into the most imporant parts of the template.\n\n&lt;div class=\"container\"&gt;\n    &lt;h1&gt;Random Messages&lt;/h1&gt;\n    &lt;ul&gt;\n        {% for message in messages %}\n            &lt;li&gt;\n                &lt;div class=\"message\"&gt;\n                    &lt;p&gt;{{ message[0] }}&lt;/p&gt; &lt;!-- name --&gt;\n                    &lt;p&gt;{{ message[1] }}&lt;/p&gt; &lt;!-- message --&gt;\n                &lt;/div&gt;\n            &lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n&lt;/div&gt;\n\nWe can see that we call the top of the page Random Messages. We then run a loop to go over all the individual messages in messages that we retrieved from our random_messages function. Then for each message we output the name and message, with indices of 0 and 1 respectively.\nWe can also look at the view_style.css page that was used for styling below:\n\n        \n        /* Additional styles specific to this template */\n        .container {\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #333; /* Darker gray container background */\n            border-radius: 10px; /* Rounded corners */\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.5); /* Shadow effect */\n        }\n        h1 {\n            text-align: center;\n            color: #ff5722; /* Orange color */\n            margin-bottom: 20px;\n            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\n            font-size: 36px; /* Larger font size */\n        }\n        ul {\n            list-style-type: none;\n            padding: 0;\n            margin: 0;\n        }\n        .message {\n            background-color: #444; /* Dark gray message background */\n            border-radius: 10px; /* Rounded corners */\n            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.3); /* Shadow effect */\n            padding: 20px;\n            margin-bottom: 20px;\n            transition: box-shadow 0.3s ease; /* Smooth transition */\n        }\n        .message:hover {\n            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.5); /* Larger shadow on hover */\n            transform: translateY(-5px); /* Move up slightly on hover */\n        }\n        p {\n            color: #ccc; /* Light gray text */\n            font-size: 18px; /* Medium font size */\n            font-style: italic; /* Italic text */\n            margin: 0; /* Remove default margin */\n        }\n\n\n\n\nNow that we have gone through the implementation on the webstie we can run through a quick demo of the site.\nWe will first look at the base page of the website:\n\n\n\nHome Page of Message Bank\n\n\nAs we can “Welcome to the Message Bank!” is displayed at the top the site, followed by the description of what the site does.\nBelow the discription, we can see two different links to click: Post Messages and View Messages. These will allow the user to move to different parts of the site.\nWe can also see that there are no routes in the url so we are at the base page and the webpage is called Message Bank, as we can see on the tab.\nAs an example lets navigate to the Post Messages page and fill in an example posted message:\n\n\n\nSubmit Page of Message Bank\n\n\nAs we can see we have naviagted to the submit page of the website. We see that the /submit route is added to the URL. The user is now able to input their name and their message. We can also see some of the style changes, as the google api font is used in this section.\nIn this example, the name is Arvin (my name) and the message is that I love basketball.\nWe can then click the submit button and it would send this tuple to the database.\nFinally we can look at the view page of the website. Let’s navigate to that:\n\n\n\nView Page of Message Bank\n\n\nAs we can see we are now in the view page of the website. We see that the route has now changed from /submit to /view. We can also see that there is a box with five random messages from the databse. All the messages have the names and the personal message attached. We see that the message that we submitted in the last page appears here as well.\n\n\n\nThe files for this website are hosted on a github repository. The repository is public so people are able to pull the code and make some modifications if they were interested.\nHere is the link to the repository for the project:\nhttps://github.com/arvinhosseini/messagebank\n\n\n\nThank you for reading my post! I hope that you were able to learn a bit about web development with flask, using a database, and CSS styling!"
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#introduction",
    "href": "posts/Flask Web Dev/index.html#introduction",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Our goal is to create a website using Flask, a python package, thats main purpose is to be a message board. We want the website to have the functionaility of having users being able to post messages to the messages board. When the users want to post a message, they should be prompted with two fields: one for their name and one for their message.\nNext, we want a user to be able to view messages of the message board as well. When the user wants to view messages they should see a defined number of random messages from the message board.\nFinally, we want to add some style to our website using CSS so the website is not boring and can be more engaging."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#set-up",
    "href": "posts/Flask Web Dev/index.html#set-up",
    "title": "Web Development with Flask",
    "section": "",
    "text": "First we need to include our neccesary packages into our main file, called app.py.\nWill also create our app, named app, that uses flask.\n\nimport sqlite3\nimport pandas as pd\n\nfrom flask import Flask, g, render_template, request\n\napp = Flask(__name__)"
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#creating-a-database",
    "href": "posts/Flask Web Dev/index.html#creating-a-database",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Next, using sqlite, we will create and host a database. This database will contain all the messages that different users submit to our message board. This database will also be used later on to display some of the messages when the user choses to view messages.\nHere is how the function, get_message_db(), is defined:\n\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        \n        # Create the messages table if it doesn't exist\n        cmd = 'CREATE TABLE IF NOT EXISTS messages (name TEXT, message TEXT);'\n        cursor.execute(cmd)\n        g.message_db.commit()\n        \n        return g.message_db\n\nWe can see that at first we try to access the database. If it not is created then we get an attribuite error so we will connect to sqlite and create our database.\nWe then create a cursor using the .cursor() function that will allow us to use SQL code to create our database.\nWe then use CREATE TABLE to make our table called messaages. This table has two columns, name and message, as these are the only things that user is able to submit to the website.\nFinally, we execute and commit the SQL code and then return the database."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#inserting-messages-into-database",
    "href": "posts/Flask Web Dev/index.html#inserting-messages-into-database",
    "title": "Web Development with Flask",
    "section": "",
    "text": "We wil next go over how to insert user messages into the database that we just created. We will do this by using the insert_messages() function. This function takes in one variable, request. From request we can access the name and the message that we want to insert into the database. We will be going how we got the request later on when we go over our routes.\nHere is the defintion for our insert_messages() function:\n\ndef insert_message(request):\n    # Get the message and the name\n    message = request.form['message']\n    name = request.form[\"name\"]\n    db = get_message_db()\n    cursor = db.cursor()\n    print(\"here\")\n    # Insert the data into our database\n    cursor.execute('INSERT INTO messages (name, message) VALUES (?, ?)', (name, message))\n    db.commit()\n    cursor.close()\n    db.close()\n    return message, name\n\nIn the function we first extract the message and name from the request and store them under the same variable names.\nNext, use our previous function to get the database and store as the variable db.\nThen we use the cursur to write an SQL command to insert the message and name as a tuple into our database.\nMake sure to close the cursor and databse, and then we return the tuple as well."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#random-messages-function",
    "href": "posts/Flask Web Dev/index.html#random-messages-function",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Our last main function we need to implement is the random messages function. The goal of this function is to retrieve an n amount of random messages from our database. We will later use this function to display these messages on the website.\nThe function is defined as the following:\n\ndef random_messages(n):\n    # Connect to database\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # Find n random messages\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    random_messages = cursor.fetchall()\n\n    # Close database\n    cursor.close()\n    db.close()\n\n    return random_messages\n\nWe can see we use the same idea as before, connecting to the database with the cursor. The main part of this function is using the ORDER BY and RANDOM() SQL commands to grab the first n tuples in the database. Since we are using RANDOM() then these n tuples will be different every time and ensures randomness.\nFinally, we close the database and return the random messages."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#routes",
    "href": "posts/Flask Web Dev/index.html#routes",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Now that we have finished writing our main three functions, we can write our routes for the website. This is the backend implementation of what is happening when the user goes onto the post message or view message page, as well as when the user decides to post a message."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#first-route-home",
    "href": "posts/Flask Web Dev/index.html#first-route-home",
    "title": "Web Development with Flask",
    "section": "",
    "text": "@app.route('/')\ndef home():\n    return render_template('base.html')\n\nWe can see that our first route is very simple. This is for when we first get onto the website, so the route is just a slash which leads to a base page. When we get onto this page we render the base.html file, which contains two options for the user: post message or view message.\nWe will go over the set up of base.html and the other templates after we finish going over the routes."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#second-route-submit",
    "href": "posts/Flask Web Dev/index.html#second-route-submit",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Next, we will write the route for the submit page, which will be /submit/. This route should allow the user to post a message onto the message board. This message should then be entered into the database.\nHere is the implementaion of the submit route:\n\n# Route to render the submit page\n@app.route('/submit', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        insert_message(request)\n        return render_template('submit.html')\n\nWe can see that within the route that we have two methods: POST and GET.\nIf the method is GET then we simply render the submit.html page. However if the method is POST, then the user wants to post a message onto the board. Therefore we use the insert_message function from earlier to insert the data into the databse. We still render the submit.html page in this method as well."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#third-route-view",
    "href": "posts/Flask Web Dev/index.html#third-route-view",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Our final route is the view route. This is for when the user wants view some of the random messages in the databse. We will access this route using /view.\nHere is the implementation of the view route:\n\n@app.route('/view')\ndef view():\n    try:\n        messages = random_messages(5)\n        return render_template('view.html', messages=messages)\n    except:\n        return render_template\n\nWe can see that we use the random_messages function with n = 5 and store the messages in the messages variable. For now we are just using n = 5 but we can always change this later. We then render the view.html page along with the messages."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#templates",
    "href": "posts/Flask Web Dev/index.html#templates",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Now that we have gone over functions and routes of our website, we will go over our main three templates."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#first-template-base",
    "href": "posts/Flask Web Dev/index.html#first-template-base",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Our first template is names base.html. This serves as the landing page for our website, so when we enter the address the site this is where we are taken. The base.html site will greet the user and ask them to either post a message onto the board or view some random messages.\nWe will also use the base.html to build our other two templates: submit and view.\nHere is the set up of base.html:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{% block title %}{% endblock %} Message Bank&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;nav&gt;\n    &lt;h1&gt;Welcome to the Message Bank!&lt;/h1&gt;\n    &lt;p&gt;Hi! Welcome to my message bank. Please feel free to post a new message by clicking on the &lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt; link. You can also view some random messages from the bank by clicking &lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;. Hope you enjoy!&lt;/p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/nav&gt;    \n\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\nWe can see that the some of this file is styling the page. This was actually really fun as I got to mess around with different styles and colors.\nHowever the most important part of this file is the following section:\n\n&lt;nav&gt;\n    &lt;h1&gt;Welcome to the Message Bank!&lt;/h1&gt;\n    &lt;p&gt;Hi! Welcome to my message bank. Please feel free to post a new message by clicking on the &lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt; link. You can also view some random messages from the bank by clicking &lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;. Hope you enjoy!&lt;/p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/nav&gt; \n\nThis section greats the user by welcoming them to the message bank and telling the user the options they can do from here.\nWe also create two links for the user “Post Messages” and “View Messages” that would lead to the submit and view routes respectively.\nTherefore once a user chooses what they want to do they are redirected to the correct page.\nWe can also view the style.css file that I used for styling the base page:\n\nbody {\n    background-color: #192841; /* Dark blue background */\n    font-family: Arial, sans-serif;\n}\nnav {\n    border-bottom: 2px solid gold;\n    padding-bottom: 20px;\n    margin-bottom: 20px;\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);\n}\nh1 {\n    color: white;\n    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\n    font-size: 28px;\n}\np {\n    color: orange;\n    font-size: 18px;\n    line-height: 1.5;\n    margin-bottom: 20px;\n}\np a {\n    color: gold;\n    text-decoration: none;\n    transition: color 0.3s ease;\n}\np a:hover {\n    color: lightgoldenrodyellow;\n}\nul {\n    list-style-type: none;\n    padding: 0;\n    margin: 0;\n}\nul li {\n    display: inline-block;\n    margin-right: 20px;\n}\nul li a {\n    color: gold;\n    font-size: 20px;\n    text-decoration: none;\n    border-bottom: 2px solid transparent;\n    transition: border-bottom-color 0.3s ease;\n}\nul li a:hover {\n    border-bottom-color: gold;\n}"
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#second-template-submit",
    "href": "posts/Flask Web Dev/index.html#second-template-submit",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Next, we have a template for the submit page called submit.html. Here the user is able to submit a message to the message bank.\nHere is the template for submit.html:\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1 style=\"color: green; font-family: 'Roboto', sans-serif; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5)\"&gt;{% block title %}&lt;span class=\"protest-guerrilla-regular\"&gt;Submit a Message&lt;/span&gt;{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n\n\n{% block content %}\n\n  &lt;style&gt;\n    /* Import Google Fonts */\n    @import url('https://fonts.googleapis.com/css2?family=Protest+Guerrilla&display=swap');\n\n    /* Apply custom font */\n    .protest-guerrilla-regular {\n      font-family: \"Protest Guerrilla\", sans-serif;\n      font-weight: 400;\n      font-style: normal;\n    }\n\n    /* Set background color */\n    body {\n      background-color:  #192841;\n    }\n  &lt;/style&gt;\n\n  &lt;form method=\"post\"&gt;\n      &lt;label for=\"name\" class=\"protest-guerrilla-regular\"&gt;What's your name?&lt;/label&gt;\n      &lt;input name=\"name\" id=\"name\" class=\"protest-guerrilla-regular\"&gt;\n      &lt;label for=\"message\" class=\"protest-guerrilla-regular\"&gt;What is your message?&lt;/label&gt;\n      &lt;input type=\"text\" name=\"message\" id=\"message\" class=\"protest-guerrilla-regular\"&gt;\n      &lt;input type=\"submit\" value=\"Post Message\" class=\"protest-guerrilla-regular\"&gt;\n  &lt;/form&gt;\n  {% if sec == True %}\n  &lt;br&gt;\n      &lt;b&gt;Message sent&lt;/b&gt;\n  {% endif %}\n{% endblock %}\n\nAgain, some of this file is the style prefernces for the page. I choose to use a google api to get the Protest Guerrilla font for some of this page.\nWe can examine the most important part of the file below:\n\n&lt;form method=\"post\"&gt;\n    &lt;label for=\"name\" class=\"protest-guerrilla-regular\"&gt;What's your name?&lt;/label&gt;\n    &lt;input name=\"name\" id=\"name\" class=\"protest-guerrilla-regular\"&gt;\n    &lt;label for=\"message\" class=\"protest-guerrilla-regular\"&gt;What is your message?&lt;/label&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\" class=\"protest-guerrilla-regular\"&gt;\n    &lt;input type=\"submit\" value=\"Post Message\" class=\"protest-guerrilla-regular\"&gt;\n&lt;/form&gt;\n\nWe can see that there are two prompts for the user: “What’s your name?” and “What is your message?”. We can see that these are then called name and message respectively. So once the user submits their info the input_message function will be called and their info will be entered into the database.\nThe html page is also based on base.html as we include {% extends ‘base.html’ %} at the top of the page. Therefore the writing of the home page is still there and the prompts show up below."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#third-template-view",
    "href": "posts/Flask Web Dev/index.html#third-template-view",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Finally, we have our third template, view.html. This page will allow the user to view the random messages from the message bank.\nHere is the implementation of the view.html template:\n\n{% extends 'base.html' %}\n\n{% block title %}\n    Random Messages\n{% endblock %}\n\n{% block content %}\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Random Messages&lt;/h1&gt;\n        &lt;ul&gt;\n            {% for message in messages %}\n                &lt;li&gt;\n                    &lt;div class=\"message\"&gt;\n                        &lt;p&gt;{{ message[0] }}&lt;/p&gt; &lt;!-- name --&gt;\n                        &lt;p&gt;{{ message[1] }}&lt;/p&gt; &lt;!-- message --&gt;\n                    &lt;/div&gt;\n                &lt;/li&gt;\n            {% endfor %}\n        &lt;/ul&gt;\n    &lt;/div&gt;\n{% endblock %}\n\n{% block header %}\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='view_style.css') }}\"&gt;\n{% endblock %}\n\nAs before, we still use base.html to extend this template and some the code is for styling. As I wanted this page to have its own style for the viewing of the message bank, I created a new file called view_style.css for the style of this page. We can take a look into the most imporant parts of the template.\n\n&lt;div class=\"container\"&gt;\n    &lt;h1&gt;Random Messages&lt;/h1&gt;\n    &lt;ul&gt;\n        {% for message in messages %}\n            &lt;li&gt;\n                &lt;div class=\"message\"&gt;\n                    &lt;p&gt;{{ message[0] }}&lt;/p&gt; &lt;!-- name --&gt;\n                    &lt;p&gt;{{ message[1] }}&lt;/p&gt; &lt;!-- message --&gt;\n                &lt;/div&gt;\n            &lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n&lt;/div&gt;\n\nWe can see that we call the top of the page Random Messages. We then run a loop to go over all the individual messages in messages that we retrieved from our random_messages function. Then for each message we output the name and message, with indices of 0 and 1 respectively.\nWe can also look at the view_style.css page that was used for styling below:\n\n        \n        /* Additional styles specific to this template */\n        .container {\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #333; /* Darker gray container background */\n            border-radius: 10px; /* Rounded corners */\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.5); /* Shadow effect */\n        }\n        h1 {\n            text-align: center;\n            color: #ff5722; /* Orange color */\n            margin-bottom: 20px;\n            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\n            font-size: 36px; /* Larger font size */\n        }\n        ul {\n            list-style-type: none;\n            padding: 0;\n            margin: 0;\n        }\n        .message {\n            background-color: #444; /* Dark gray message background */\n            border-radius: 10px; /* Rounded corners */\n            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.3); /* Shadow effect */\n            padding: 20px;\n            margin-bottom: 20px;\n            transition: box-shadow 0.3s ease; /* Smooth transition */\n        }\n        .message:hover {\n            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.5); /* Larger shadow on hover */\n            transform: translateY(-5px); /* Move up slightly on hover */\n        }\n        p {\n            color: #ccc; /* Light gray text */\n            font-size: 18px; /* Medium font size */\n            font-style: italic; /* Italic text */\n            margin: 0; /* Remove default margin */\n        }"
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#demonstration",
    "href": "posts/Flask Web Dev/index.html#demonstration",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Now that we have gone through the implementation on the webstie we can run through a quick demo of the site.\nWe will first look at the base page of the website:\n\n\n\nHome Page of Message Bank\n\n\nAs we can “Welcome to the Message Bank!” is displayed at the top the site, followed by the description of what the site does.\nBelow the discription, we can see two different links to click: Post Messages and View Messages. These will allow the user to move to different parts of the site.\nWe can also see that there are no routes in the url so we are at the base page and the webpage is called Message Bank, as we can see on the tab.\nAs an example lets navigate to the Post Messages page and fill in an example posted message:\n\n\n\nSubmit Page of Message Bank\n\n\nAs we can see we have naviagted to the submit page of the website. We see that the /submit route is added to the URL. The user is now able to input their name and their message. We can also see some of the style changes, as the google api font is used in this section.\nIn this example, the name is Arvin (my name) and the message is that I love basketball.\nWe can then click the submit button and it would send this tuple to the database.\nFinally we can look at the view page of the website. Let’s navigate to that:\n\n\n\nView Page of Message Bank\n\n\nAs we can see we are now in the view page of the website. We see that the route has now changed from /submit to /view. We can also see that there is a box with five random messages from the databse. All the messages have the names and the personal message attached. We see that the message that we submitted in the last page appears here as well."
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#github-repo",
    "href": "posts/Flask Web Dev/index.html#github-repo",
    "title": "Web Development with Flask",
    "section": "",
    "text": "The files for this website are hosted on a github repository. The repository is public so people are able to pull the code and make some modifications if they were interested.\nHere is the link to the repository for the project:\nhttps://github.com/arvinhosseini/messagebank"
  },
  {
    "objectID": "posts/Flask Web Dev/index.html#conclusion",
    "href": "posts/Flask Web Dev/index.html#conclusion",
    "title": "Web Development with Flask",
    "section": "",
    "text": "Thank you for reading my post! I hope that you were able to learn a bit about web development with flask, using a database, and CSS styling!"
  },
  {
    "objectID": "posts/Tutorial/index.html",
    "href": "posts/Tutorial/index.html",
    "title": "Data Visualization Tutorial",
    "section": "",
    "text": "Hello! Welcome to my Data Visualization Tutorial. For the tutorial, I will be using the Palmer Penguins data set. The url for this data set can be found below.\nThe first step is to import pandas and load in the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Tutorial/index.html#intro-and-set-up",
    "href": "posts/Tutorial/index.html#intro-and-set-up",
    "title": "Data Visualization Tutorial",
    "section": "",
    "text": "Hello! Welcome to my Data Visualization Tutorial. For the tutorial, I will be using the Palmer Penguins data set. The url for this data set can be found below.\nThe first step is to import pandas and load in the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Tutorial/index.html#using-head-and-info-functions",
    "href": "posts/Tutorial/index.html#using-head-and-info-functions",
    "title": "Data Visualization Tutorial",
    "section": "Using Head() and Info() Functions",
    "text": "Using Head() and Info() Functions\nNext, we can look at what the data looks like using the head and describe function in pandas.\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   studyName            344 non-null    object \n 1   Sample Number        344 non-null    int64  \n 2   Species              344 non-null    object \n 3   Region               344 non-null    object \n 4   Island               344 non-null    object \n 5   Stage                344 non-null    object \n 6   Individual ID        344 non-null    object \n 7   Clutch Completion    344 non-null    object \n 8   Date Egg             344 non-null    object \n 9   Culmen Length (mm)   342 non-null    float64\n 10  Culmen Depth (mm)    342 non-null    float64\n 11  Flipper Length (mm)  342 non-null    float64\n 12  Body Mass (g)        342 non-null    float64\n 13  Sex                  334 non-null    object \n 14  Delta 15 N (o/oo)    330 non-null    float64\n 15  Delta 13 C (o/oo)    331 non-null    float64\n 16  Comments             26 non-null     object \ndtypes: float64(6), int64(1), object(10)\nmemory usage: 45.8+ KB\n\n\nAs we can see from the displayed info, there are 344 entries in the dataframe as well as 17 different columns that are associated with each entry. So each data point has a studyName, Sample Number, ect. We can see that some columns, such as Comments or Body Mass, have a non-Null count of under 344. This means that some data entries in the dataframe do not have any values for some of these columns.\nFor this tuturial, lets look at the following four columns :‘culmen length’, ‘culmen depth’, ‘flipper length’, ‘body mass’ and see how they compare based on the study. Since some of these are null, lets remove the rows that have a null value for at least one of these columns.\n\ncolumns_of_interest = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\ndf_cleaned = penguins.dropna(subset=columns_of_interest)\n\n\ndf_cleaned.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 342 entries, 0 to 343\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   studyName            342 non-null    object \n 1   Sample Number        342 non-null    int64  \n 2   Species              342 non-null    object \n 3   Region               342 non-null    object \n 4   Island               342 non-null    object \n 5   Stage                342 non-null    object \n 6   Individual ID        342 non-null    object \n 7   Clutch Completion    342 non-null    object \n 8   Date Egg             342 non-null    object \n 9   Culmen Length (mm)   342 non-null    float64\n 10  Culmen Depth (mm)    342 non-null    float64\n 11  Flipper Length (mm)  342 non-null    float64\n 12  Body Mass (g)        342 non-null    float64\n 13  Sex                  334 non-null    object \n 14  Delta 15 N (o/oo)    330 non-null    float64\n 15  Delta 13 C (o/oo)    331 non-null    float64\n 16  Comments             25 non-null     object \ndtypes: float64(6), int64(1), object(10)\nmemory usage: 48.1+ KB\n\n\nWe can see that two rows got dropped from the dataframe, as the total number of entries is now 342."
  },
  {
    "objectID": "posts/Tutorial/index.html#visualization-using-box-plot",
    "href": "posts/Tutorial/index.html#visualization-using-box-plot",
    "title": "Data Visualization Tutorial",
    "section": "Visualization using Box Plot",
    "text": "Visualization using Box Plot\nNext, we can use matplotlib to visualize some of the data, using these four columns that we are intersted in.\n\n#import matplotlib\nimport matplotlib.pyplot as plt\n\nNow we can write a basic for loop that displays the box plot for each of these four columns.\n\nfor column in columns_of_interest:\n    plt.figure(figsize=(8, 5))\n    df_cleaned[column].plot(kind='box', vert=False)\n    plt.title(f'Box Plot for {column}')\n    plt.xlabel(column)\n    plt.show()"
  },
  {
    "objectID": "posts/Tutorial/index.html#visualization-with-seaborn-and-scatterplots",
    "href": "posts/Tutorial/index.html#visualization-with-seaborn-and-scatterplots",
    "title": "Data Visualization Tutorial",
    "section": "Visualization with Seaborn and Scatterplots",
    "text": "Visualization with Seaborn and Scatterplots\nWe can also use a software seaborn, a data visualization tool, help with our data.\n\nimport seaborn as sns\n\nWe can use seaborn to display scatter plots that compare each of the columns againist each other in order to find certain trends.\n\nsns.pairplot(df_cleaned[columns_of_interest])\nplt.suptitle('Scatter Plots for Culmen Length, Culmen Depth, Flipper Length, and Body Mass', y=1.02)\nplt.show()\n\n\n\n\nAs we can see there are a total of 16 plots shown, and when we have a case of the column vs itself, a bar graph is displayed which shows the amount of category. From these charts we can make some insights. For example we can see that there is a relationship between flipper length and body mass. From the scatter plot, we can see that as flipper length increases, so does the body mass."
  },
  {
    "objectID": "posts/Tutorial/index.html#conclusion",
    "href": "posts/Tutorial/index.html#conclusion",
    "title": "Data Visualization Tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nThank you for reading this post!"
  },
  {
    "objectID": "posts/Image Classification/index.html",
    "href": "posts/Image Classification/index.html",
    "title": "Image Classification of Cats and Dogs",
    "section": "",
    "text": "Hello! Welcome to my blog post where we will be going over image classification. We will use the Keras library to build and train our models. Our goal is to create a model that will successfully classify pictures of random cats and dogs correctly."
  },
  {
    "objectID": "posts/Image Classification/index.html#intro",
    "href": "posts/Image Classification/index.html#intro",
    "title": "Image Classification of Cats and Dogs",
    "section": "",
    "text": "Hello! Welcome to my blog post where we will be going over image classification. We will use the Keras library to build and train our models. Our goal is to create a model that will successfully classify pictures of random cats and dogs correctly."
  },
  {
    "objectID": "posts/Image Classification/index.html#set-up-and-data-loading",
    "href": "posts/Image Classification/index.html#set-up-and-data-loading",
    "title": "Image Classification of Cats and Dogs",
    "section": "Set Up and Data Loading",
    "text": "Set Up and Data Loading\nOur first step is to import the all the packages and modules we will need throughout this process.\n\nimport os\nimport keras\nimport numpy as np\nfrom keras import utils\nimport tensorflow_datasets as tfds\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport matplotlib.pyplot as plt\n\nNext we will load in our data and split into training, validation, and test sets.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nWe can see that we have 9305 training images, as well as 2326 validation and test images.\nNext we need to resize, since some of the images have differnt dimensions. After running this code, all the images would be 150 by 150.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nFinally, we need to run this block of code to rapidly read the data.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/Image Classification/index.html#visualization",
    "href": "posts/Image Classification/index.html#visualization",
    "title": "Image Classification of Cats and Dogs",
    "section": "Visualization",
    "text": "Visualization\nNow that we have set up all our data, we can see what our data actually consists of.\nThe function visualize_samples(), allows to see 3 random images from the cat data and 3 random images from the dog data.\n\ndef visualize_samples(dataset, num_samples=6):\n    plt.figure(figsize=(15, 7))\n    plt.suptitle('Sample Images from the Dataset')\n\n    images_per_class = num_samples // 2  # Number of images per class (cats and dogs)\n\n    # Iterate through each batch in the dataset\n    for images, labels in dataset.take(1):  # Only take the first batch for visualization\n        cat_images = images[labels == 0][:images_per_class]\n        dog_images = images[labels == 1][:images_per_class]\n\n        # Show cat images\n        for i in range(images_per_class):\n            plt.subplot(2, images_per_class, i + 1)\n            plt.imshow(cat_images[i].numpy().astype(\"uint8\"))  # Convert to uint8 for visualization\n            plt.title(\"Cat\")\n            plt.axis(\"off\")\n\n        # Show dog images\n        for i in range(images_per_class):\n            plt.subplot(2, images_per_class, images_per_class + i + 1)\n            plt.imshow(dog_images[i].numpy().astype(\"uint8\"))  # Convert to uint8 for visualization\n            plt.title(\"Dog\")\n            plt.axis(\"off\")\n\n    plt.show()\n\nLet’s run the function on our training data, train_ds.\n\nvisualize_samples(train_ds)\n\n\n\n\nWe can see that the first row of the output is three different images of cats and the second row of the output is three different iamges of dogs."
  },
  {
    "objectID": "posts/Image Classification/index.html#third-model-data-preprocessing",
    "href": "posts/Image Classification/index.html#third-model-data-preprocessing",
    "title": "Image Classification of Cats and Dogs",
    "section": "Third Model: Data Preprocessing",
    "text": "Third Model: Data Preprocessing\nIn our next model we will use data preprocessing to try to improve our accuracy.\nWe will run the following block of code to create a layer called preprocessor.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nNext we will add this layer to previous model and name the model model3.\n\n# Define the model architecture\nmodel3 = keras.Sequential([\n    # Add the Preprocessor layer\n    preprocessor,\n\n    # Random Flip Layer\n    random_flip_layer,\n\n    # Random Rotation Layer\n    random_rotation_layer,\n\n    # Convolutional layers\n    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3)),\n    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n\n    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n\n    # Add another convoluational and pooling layer to improve model\n    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n\n    # Flatten layer to convert 2D features to 1D feature vector\n    keras.layers.Flatten(),\n\n    # Dense (fully connected) layers\n    keras.layers.Dense(256, activation='relu'),\n\n    # Dropout layer to prevent overfitting\n    keras.layers.Dropout(0.3),\n\n    # Output layer with sigmoid activation for binary classification\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nWe can see that we the preproccessor layer first. Also in order to improve the models performance, I added another convoluation and pooling layer.\nLet’s train this new model and see if we have any improvement.\n\nhistory3 = model3.fit(train_ds, epochs=20, validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 8s 41ms/step - loss: 0.6869 - accuracy: 0.5730 - val_loss: 0.6493 - val_accuracy: 0.6354\nEpoch 2/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.6479 - accuracy: 0.6296 - val_loss: 0.6037 - val_accuracy: 0.6926\nEpoch 3/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.6091 - accuracy: 0.6765 - val_loss: 0.5644 - val_accuracy: 0.7214\nEpoch 4/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.5857 - accuracy: 0.6909 - val_loss: 0.5493 - val_accuracy: 0.7244\nEpoch 5/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.5674 - accuracy: 0.7070 - val_loss: 0.5416 - val_accuracy: 0.7296\nEpoch 6/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.5480 - accuracy: 0.7255 - val_loss: 0.5216 - val_accuracy: 0.7446\nEpoch 7/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.5336 - accuracy: 0.7280 - val_loss: 0.5267 - val_accuracy: 0.7472\nEpoch 8/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.5271 - accuracy: 0.7380 - val_loss: 0.5059 - val_accuracy: 0.7627\nEpoch 9/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.5169 - accuracy: 0.7420 - val_loss: 0.5004 - val_accuracy: 0.7678\nEpoch 10/20\n146/146 [==============================] - 6s 39ms/step - loss: 0.5116 - accuracy: 0.7481 - val_loss: 0.5070 - val_accuracy: 0.7584\nEpoch 11/20\n146/146 [==============================] - 5s 37ms/step - loss: 0.4975 - accuracy: 0.7562 - val_loss: 0.4962 - val_accuracy: 0.7700\nEpoch 12/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.4971 - accuracy: 0.7616 - val_loss: 0.4777 - val_accuracy: 0.7764\nEpoch 13/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.4936 - accuracy: 0.7609 - val_loss: 0.4893 - val_accuracy: 0.7717\nEpoch 14/20\n146/146 [==============================] - 6s 40ms/step - loss: 0.4827 - accuracy: 0.7693 - val_loss: 0.4853 - val_accuracy: 0.7760\nEpoch 15/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.4790 - accuracy: 0.7722 - val_loss: 0.4783 - val_accuracy: 0.7769\nEpoch 16/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.4680 - accuracy: 0.7798 - val_loss: 0.4761 - val_accuracy: 0.7782\nEpoch 17/20\n146/146 [==============================] - 5s 38ms/step - loss: 0.4630 - accuracy: 0.7780 - val_loss: 0.4666 - val_accuracy: 0.7829\nEpoch 18/20\n146/146 [==============================] - 6s 38ms/step - loss: 0.4509 - accuracy: 0.7870 - val_loss: 0.4731 - val_accuracy: 0.7906\nEpoch 19/20\n146/146 [==============================] - 6s 40ms/step - loss: 0.4499 - accuracy: 0.7909 - val_loss: 0.4722 - val_accuracy: 0.7923\nEpoch 20/20\n146/146 [==============================] - 5s 38ms/step - loss: 0.4454 - accuracy: 0.7923 - val_loss: 0.4587 - val_accuracy: 0.8001\n\n\n\nvalidation_accuracy = history3.history['val_accuracy'][-1]\nprint(\"Validation Accuracy:\", validation_accuracy)\n\nValidation Accuracy: 0.8000859618186951\n\n\nAftering trainig the model, we see that we have a validation accuracy of 80%.\nThis is a signifant jump in performance from our previous two models, as this validation accuracy is about 16% higher than that from model1.\nWe also see that there is very little to no overfitting. We can see that this is true since the validation and training accuracies are very similar to each through the epochs."
  },
  {
    "objectID": "posts/Image Classification/index.html#fourth-model-transfer-learning",
    "href": "posts/Image Classification/index.html#fourth-model-transfer-learning",
    "title": "Image Classification of Cats and Dogs",
    "section": "Fourth Model: Transfer Learning",
    "text": "Fourth Model: Transfer Learning\nFor this model, we will try to use a pre-existing model that was already trained to classify dogs and cats. We will incorporate this model into our fourth and final model.\nWe run the code below to download MobileNetV3Large and create the base_model_layer that we will use in our final model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 2s 0us/step\n\n\nNow let’s define model4. We will still use data augmentation layer’s from our previous two models. Then we add our base_model_layer that we obtained from the code above. Next we add convoluational layer to imporve performance, as well as a GlobalMaxPooling layer and a dense layer.\n\n# Define the model architecture\nmodel4 = keras.Sequential([\n\n\n    # Random Flip Layer\n    random_flip_layer,\n\n    # Random Rotation Layer\n    random_rotation_layer,\n\n    # Add the base layer\n    base_model_layer,\n\n    # Add convulational layer\n    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n\n    # Global max pooling layer\n    keras.layers.GlobalMaxPooling2D(),\n\n\n    # Dense layer\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nNow we can train our final model and see if the base model improved on our accuracy.\n\nhistory4 = model4.fit(train_ds, epochs=20, validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 15s 54ms/step - loss: 0.5440 - accuracy: 0.8752 - val_loss: 0.1075 - val_accuracy: 0.9617\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1826 - accuracy: 0.9253 - val_loss: 0.1026 - val_accuracy: 0.9647\nEpoch 3/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.1673 - accuracy: 0.9319 - val_loss: 0.0936 - val_accuracy: 0.9656\nEpoch 4/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1476 - accuracy: 0.9394 - val_loss: 0.1065 - val_accuracy: 0.9635\nEpoch 5/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.1407 - accuracy: 0.9438 - val_loss: 0.0865 - val_accuracy: 0.9712\nEpoch 6/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1333 - accuracy: 0.9455 - val_loss: 0.0862 - val_accuracy: 0.9652\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1246 - accuracy: 0.9507 - val_loss: 0.0877 - val_accuracy: 0.9665\nEpoch 8/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1221 - accuracy: 0.9515 - val_loss: 0.0856 - val_accuracy: 0.9712\nEpoch 9/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1149 - accuracy: 0.9559 - val_loss: 0.0868 - val_accuracy: 0.9695\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1187 - accuracy: 0.9538 - val_loss: 0.0967 - val_accuracy: 0.9656\nEpoch 11/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1164 - accuracy: 0.9550 - val_loss: 0.0871 - val_accuracy: 0.9708\nEpoch 12/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.1001 - accuracy: 0.9580 - val_loss: 0.0899 - val_accuracy: 0.9699\nEpoch 13/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.1017 - accuracy: 0.9612 - val_loss: 0.0780 - val_accuracy: 0.9703\nEpoch 14/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0982 - accuracy: 0.9601 - val_loss: 0.0793 - val_accuracy: 0.9729\nEpoch 15/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.1021 - accuracy: 0.9613 - val_loss: 0.0919 - val_accuracy: 0.9682\nEpoch 16/20\n146/146 [==============================] - 7s 44ms/step - loss: 0.0928 - accuracy: 0.9640 - val_loss: 0.0893 - val_accuracy: 0.9690\nEpoch 17/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0918 - accuracy: 0.9657 - val_loss: 0.0872 - val_accuracy: 0.9686\nEpoch 18/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.0886 - accuracy: 0.9661 - val_loss: 0.0827 - val_accuracy: 0.9716\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.0859 - accuracy: 0.9672 - val_loss: 0.0990 - val_accuracy: 0.9652\nEpoch 20/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.0953 - accuracy: 0.9632 - val_loss: 0.0851 - val_accuracy: 0.9716\n\n\n\nvalidation_accuracy = history4.history['val_accuracy'][-1]\nprint(\"Validation Accuracy:\", validation_accuracy)\n\nValidation Accuracy: 0.9716250896453857\n\n\nWe can see that this model has validation accuracy of 97.16%.\nThis is a large improvement from our previous model, which had validation accuracy of 80%. Thus, this new model had a 17% improvement in performance from model3.\nAgain, we see little to no overfitting in this model, as the training and validation do not differ much throughout training.\n\nmodel4.summary()\n\nModel: \"sequential_19\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_flip_7 (RandomFlip)  (None, 150, 150, 3)       0         \n                                                                 \n random_rotation_8 (RandomR  (None, 150, 150, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n conv2d_29 (Conv2D)          (None, 3, 3, 32)          276512    \n                                                                 \n global_max_pooling2d_9 (Gl  (None, 32)                0         \n obalMaxPooling2D)                                               \n                                                                 \n dense_29 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3272897 (12.49 MB)\nTrainable params: 276545 (1.05 MB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\nWe can use the model.summary() to take a deeper look into our model. We can see that the base layer we added to our new model is indeed very complex. We can see that this layer took in 2996352 parameters. This is significantly more than any other layer in any of our other models. Therefore we can see why we achieved such a high validation score, as this newly imported layer is very complex."
  },
  {
    "objectID": "posts/Image Classification/index.html#outro",
    "href": "posts/Image Classification/index.html#outro",
    "title": "Image Classification of Cats and Dogs",
    "section": "Outro",
    "text": "Outro\nThank you for reading my blog post! I hope you were able to learn new information about Keras and image classification!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello! Here is my welcome post for my new blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html",
    "href": "posts/Movie Web Scrapping/index.html",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "Hello! In this blog post I will go over how to create a web scrapper using scrapy. For the purpose of this post, the goal of our web scrapper is to find all actors from a particular movie and output the list the actors along with all other movies or TV shows they have been in. The website we will be webscrapping is www.themoviedb.org, which is a website that holds movie and TV show information.\n\n\nThe first step is to include the neccesary packages. When I was creating this webscrapper, I had issues with 403 errors. Therefore, I used a proxy via ScrapeOps to help resolve the errors and successfully scrape the websites.\n\nimport scrapy\nfrom urllib.parse import urlencode\n\nAPI_KEY = \"816aed8f-f731-424c-b089-7c03af87389e\"\n\n\ndef get_scrapeops_url(url):\n    payload = {'api_key': API_KEY, 'url': url}\n    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n    return proxy_url\n\nThe get_scrapeops_url function will serve as a proxy and we can input a normal url into the function and it allow for us to not get flagged.\n\n\n\nNow that we have inlcuded everything neccessary, we can start building our Spider class from scrapy. First we need to declare the class and the definite the constructor function init which would define the starting url that we are trying to scrape.\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [get_scrapeops_url(f\"https://www.themoviedb.org/movie/{subdir}/\")]\n\nAs we can see, the we created the TmbdSpider class and named it ‘tmbd_spider’ and used the proxy to build our constructor. The {subdir} portion of the URL is what movie we are trying to scrape. For example, my favorie moive is Interstellar, so subdir will be defined as 157336-interstellar, since the full URL is https://www.themoviedb.org/movie/157336-interstellar.\nNext, we need to define our parse function, which will take our spider web scrapper to the cast and crew section of our movie.\n\ndef parse(self, response):\n   \"\"\"\n   Parses the response from the main page of a movie on www.themoviedb.org.\n\n   Assumptions:\n   - Assumes that the response is from the main page of a movie on www.themoviedb.org.\n\n   Effects:\n   - Navigates to the Full Cast & Crew page of the movie.\n   - Yields a scrapy.Request to parse the full credits page of the movie.\n\n   Args:\n      response: The response object containing the main page's HTML.\n\n   Yields:\n      scrapy.Request: A request to parse the full credits page of the movie.\n   \"\"\"\n   # Navigate to the Full Cast & Crew page\n   full_credits_url = response.url + '/cast/'\n   yield scrapy.Request(full_credits_url , callback=self.parse_full_credits)\n\nOn themoviedb webstite, once we are on our movie’s website (i.e. https://www.themoviedb.org/movie/157336-interstellar), if we click on the Full Cast and Crew button we are redirected to the cast page with the following URL: https://www.themoviedb.org/movie/157336-interstellar/cast. As we can see, the only difference between this URL and the previous is the added /cast at the end. Therefore, we will yeid the same URL as before and make sure to add the /cast/ the URL, so that we are now navigated to the Full Cast and Crew page.\n\n\n\nNext we will implement the parse_full_credits function of our spider class. The goal of this function will be to retrieve all the actor links for the actors in our movie. On the movie database website, each actor has their own page which lists information about thier bio, acting credits, and more. For example, the lead actor in Interstellar has the following page on TMDB: https://www.themoviedb.org/person/10297-matthew-mcconaughey. Here is the implementation of the parse_full_credits method:\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Parses the full credits page of a movie on www.themoviedb.org to retrieve actor links.\n\n        Assumptions:\n        - Assumes that the response is from the full credits page of a movie on www.themoviedb.org.\n\n        Effects:\n        - Extracts actor links from the page.\n        - Yields scrapy.Requests to parse each actor's page.\n\n        Args:\n            response: The response object containing the full credits page's HTML.\n\n        Yields:\n            scrapy.Request: Requests to parse each actor's page.\n        \"\"\"\n        # Extract actor links and yield requests for each actor's page\n        actor_links = response.css('div.info &gt; p &gt; a::attr(href)').getall()\n        for actor_link in actor_links:\n            yield scrapy.Request(get_scrapeops_url(\"https://www.themoviedb.org\" + actor_link), callback=self.parse_actor_page)\n\nIf we look at few of the pages on TMDB website, we notice that every acting page had the same starting URL: https://www.themoviedb.org. Therefore, we need to find what follows this URL, which we call actor_link, then add it to the starting URL, which will give the full URL for the acting page.\nBy going on the initial movie’s website and using the developer’s tools, I was able to find that link to every actor in the movie was listed in there. By following the path from div.info to p to a::attr(href), we are able to extract the second half the URL that we need.\nThus, we can yield the full URL for each actor in the movie.\n\n\n\nFinally, we need to implement our final function, the parse_actor_page function. The goal of this function to go through each of the actor’s page and extract all the movies and TV shows that they have appeared in. We will then yield the name of the actor and the list of movies and TV shows.\nLet’s first go over how to extract the name of the actor. The beggining of the parse_actor_page looks like this:\n\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Parses an actor's page on www.themoviedb.org to retrieve their name and movie/TV show credits.\n\n        Assumptions:\n        - Assumes that the response is from an actor's page on www.themoviedb.org.\n\n        Effects:\n        - Extracts the actor's name and their movie/TV show credits.\n        - Yields the actor's name and their credited movie/TV show.\n\n        Args:\n            response: The response object containing the actor's page's HTML.\n\n        Yields:\n            dict: A dictionary containing the actor's name and their credited movie/TV show.\n        \"\"\"\n\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\nUsing the developer’s tools again, I was able to see that every actor’s page had their name listed in the source code under, first under the head section, then under the title section. Therefore, the selector response.css(‘head &gt; title::text’).get(), would retrieve the title of the webpage. However, this response includes the title of the website. So for example, for Matthew McConaughey, the selector would give the following response: “Matthew McConaughey — The Movie Database (TMDB)”. Therefore we need to cut off everything from the long dash,‘—’, and onwards. By using the split function with ‘—’ and selecting the first half the split, [0], we are able to isolate the name of the actor. We will store this value in the variable actor_name.\nNext, we need to implement the second half the function that will extract the movie and TV names for every actor. Here is the full implementation of the parse_actor_page function, with the second half of the function:\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Parses an actor's page on www.themoviedb.org to retrieve their name and movie/TV show credits.\n\n        Assumptions:\n        - Assumes that the response is from an actor's page on www.themoviedb.org.\n\n        Effects:\n        - Extracts the actor's name and their movie/TV show credits.\n        - Yields the actor's name and their credited movie/TV show.\n\n        Args:\n            response: The response object containing the actor's page's HTML.\n\n        Yields:\n            dict: A dictionary containing the actor's name and their credited movie/TV show.\n        \"\"\"\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\n       # Find the index of 'Acting' in the list of credits\n        acting_index = response.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\n        # Select the element corresponding to 'Acting' and its associated table\n        acting_only = response.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\n        # Extract movie or TV names from the selected table\n        movie_or_TV_name = acting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\n\n        for movie_or_TV_name in movie_or_TV_name:\n            yield {\n                \"actor\":actor_name,\n                \"movie_or_TV_name\": movie_or_TV_name\n            }\n\nThis part was a bit tricky as we want to make sure that we only select movies and TV shows in which the person’s page that we are personing were credited in an acting role, as some also have production and other credits that we do not want included in the list.\nIn order to do this we must take advantage of the indexing in source code that will allow us to select only the acting roles. After looking an the source code on the page, we can see that all the acting roles for an actor had a line in common before listing all acting roles:\n&lt;h3 class=“zero”&gt;Acting&lt;&gt;\n\nTherefore we can use the follwoing xpath response to find the index for each actor:\n\nresponse.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\nWe will store this index under the variable name actor_index.\nNext we must use this variable to find the table on the actors page that corresponds to the acting index. Thus we can use the following to return this section of the table under the acting index.\n\nresponse.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\nThe reason we must add the +1 to acting_index is that xpath starts its index at 1 instead of 0. Therefore we must account for this change by adding the one.\nThe final stage of our extraction is to go through the table we selected and retrieve the title of each movie or TV show that the actor appears in.\nUsing the inspect element tool again, we can see that information is located under the &lt;a class = “tooltip”&gt; section, with the text of the film being wrapped by the &lt;bdi&gt; markers.\nTherefore, we can use the followig line to get all the titles of the films and shows, as we are already under the acting table of the page. Therefore movie_or_TV_name will be a list of the credits for each actor.\n\nacting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\nFinally, we yield the actors name and the credited film or TV show under the names “actor” and “movie_or_TV_show”, so when we export to a CSV file they will be the name of our columns.\nNow that we have completed our spider class, we can run our code and output to a CSV file called output. Therefore we must run the following line in our terminal.\nscrapy crawl tmdb_spider -o output.csv -a subdir=157336-interstellar\nThis will create an output called output.csv, we can read this file into a Pandas dataframe to view our data.\n\n\n\nNow that we have our output we can convert the csv file into a Pandas dataframe to view and analyze our data. In a seperate file we can import Pandas and run the following line of code:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"output.csv\")\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nAs we can see we have 2063 rows of movie and TV show titles, with the columns actor and movie_or_TV_name.\nHere is a what our data table would look like:\n\ndf.head(10)\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nMatthew McConaughey\n2024\n\n\n1\nMatthew McConaughey\nThe Lost Bus\n\n\n2\nMatthew McConaughey\nThe Rivals of Amziah King\n\n\n3\nMatthew McConaughey\nAgent Elvis\n\n\n4\nMatthew McConaughey\nThe Jennifer Hudson Show\n\n\n5\nMatthew McConaughey\nDeep in the Heart: A Texas Wildlife Story\n\n\n6\nMatthew McConaughey\nSing 2\n\n\n7\nMatthew McConaughey\nCome Home\n\n\n8\nMatthew McConaughey\n05\n\n\n9\nMatthew McConaughey\nRoll Up Your Sleeves\n\n\n\n\n\n\n\nWe can see that Matthew McConaughey is the first actor in our list and we can see his associated roles, for example “The Lost Bus” and “Come Home” are a couple of his roles.\nNext we can see compute a sorted list with the top movies and TV shows that share actors from the movie Interstellar.\nFirst we need to count the data by the tile of the role:\n\ncounted = df[\"movie_or_TV_name\"].value_counts()\ncounted\n\nmovie_or_TV_name\nInterstellar                   35\nSaturday Night Live            10\nThe View                        9\nLate Night with Seth Meyers     9\nInside 'Interstellar'           8\n                               ..\nMayo                            1\nAs You Like It                  1\nBorn Equal                      1\nFive Days                       1\nThree Below Zero                1\nName: count, Length: 1754, dtype: int64\n\n\nAs we can see the movie that we based our web crawler on appears first, so we should drop that from the list.\n\ncounted = counted.drop(counted.index[0])\n\nNext, in order to for us to work with this data easier we must convert the Pandas series into a Pandas Data Frame and reset the index.\n\ncounted = counted.to_frame()\ncounted.reset_index(inplace=True)\n\nWe will also renmae the columns to Movie or Show Title and Number of Shared Actors.\n\ncounted = counted.rename(columns={\"movie_or_TV_name\":\"Movie or Show Title\", \"count\":\"Number of Shared Actors\"})\n\nThus our new dataframe looks like the following:\n\ncounted\n\n\n\n\n\n\n\n\nMovie or Show Title\nNumber of Shared Actors\n\n\n\n\n0\nSaturday Night Live\n10\n\n\n1\nThe View\n9\n\n\n2\nLate Night with Seth Meyers\n9\n\n\n3\nInside 'Interstellar'\n8\n\n\n4\nThe Tonight Show Starring Jimmy Fallon\n8\n\n\n...\n...\n...\n\n\n1748\nMayo\n1\n\n\n1749\nAs You Like It\n1\n\n\n1750\nBorn Equal\n1\n\n\n1751\nFive Days\n1\n\n\n1752\nThree Below Zero\n1\n\n\n\n\n1753 rows × 2 columns\n\n\n\nNow we can use plotly to create a bar graph of the some of the most common shared movies or TV shows among the actors in Interstellar. Since there are 1753 movies or TV shows, we will only pick the top 15 titles to display.\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nimport plotly.express as px\n\ntop_counted = counted.head(15)\n\n# Create a bar chart using Plotly\nfig = px.bar(top_counted, x='Movie or Show Title', y='Number of Shared Actors', title='Number of Shared Actors per Movie or Show')\n\n# Show the plot\nfig.show()\n\n\n\n\nAs we can see, the top shared movies and shows are displayed in an interactive bar graph format, where if we hover over the bar we can exactly how many shared actors there are. The results are somewhat expected as well. We can see a majority of the shared titles are from late night shows or talk shows. This makes sense, as actors and celebrities often appear on these shows. Since Interstellar has a good amount of famous actors that would appear on these shows, the top results are something that we expect.\nOther top shared roles are Inside “Interstellar”, which is a documentary on how the film was made, so it would make sense that a lot of the actors from Interstellar would appear in the documentary.\nAnother film of note is The Dark Knight Rises. This film, along with Interstellar, was directed by Christopher Nolan, and he is know for using some of the same actors in his films. Therefore it is not suprising to see one of his films on this list.\n\n\n\nFor our second data analysis, we can find the number of roles that each actor has been in and then display a bar graph to visualize the data.\nWe will count the number of roles and then sort by highest of number of roles. We will use plotly again to produce the graph.\n\n# Count number of roles and sort by highest number\nroles_count = df.groupby('actor').size().reset_index(name='Number of Roles')\n\nroles_count = roles_count.sort_values(by='Number of Roles', ascending=False)\n\n# Create a bar chart using Plotly\nfig = px.bar(roles_count, x='actor', y='Number of Roles', title='Number of Roles for Each Actor')\n\n# Show the plot\nfig.show()\n\n\n\n\nWe can see that the interactive bar graph is displayed. The results are also what we expected. The actor with (by far) the highest number of roles is Michael Caine, with 224 roles. This is not suprising as Michael Caine is one of the oldest actors and has had a very long career in the film industry.\nWe can also see Matt Damon, Matthew McConaughey, and Anne Hathaway have the fourth, fifth, and sixth top roles. These are famous contemporary actors who have appeared in many famous films as well as have had very successfull careers. Therefore it is not suprising to see them near the top of the list.\n\n\n\nThank you for reading my blog post! I hope that you were able to learn more about web scrapping as well as how to display scrapped data."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#set-up",
    "href": "posts/Movie Web Scrapping/index.html#set-up",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "The first step is to include the neccesary packages. When I was creating this webscrapper, I had issues with 403 errors. Therefore, I used a proxy via ScrapeOps to help resolve the errors and successfully scrape the websites.\n\nimport scrapy\nfrom urllib.parse import urlencode\n\nAPI_KEY = \"816aed8f-f731-424c-b089-7c03af87389e\"\n\n\ndef get_scrapeops_url(url):\n    payload = {'api_key': API_KEY, 'url': url}\n    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n    return proxy_url\n\nThe get_scrapeops_url function will serve as a proxy and we can input a normal url into the function and it allow for us to not get flagged."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#declaring-class-and-defining-parse-function",
    "href": "posts/Movie Web Scrapping/index.html#declaring-class-and-defining-parse-function",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "Now that we have inlcuded everything neccessary, we can start building our Spider class from scrapy. First we need to declare the class and the definite the constructor function init which would define the starting url that we are trying to scrape.\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [get_scrapeops_url(f\"https://www.themoviedb.org/movie/{subdir}/\")]\n\nAs we can see, the we created the TmbdSpider class and named it ‘tmbd_spider’ and used the proxy to build our constructor. The {subdir} portion of the URL is what movie we are trying to scrape. For example, my favorie moive is Interstellar, so subdir will be defined as 157336-interstellar, since the full URL is https://www.themoviedb.org/movie/157336-interstellar.\nNext, we need to define our parse function, which will take our spider web scrapper to the cast and crew section of our movie.\n\ndef parse(self, response):\n   \"\"\"\n   Parses the response from the main page of a movie on www.themoviedb.org.\n\n   Assumptions:\n   - Assumes that the response is from the main page of a movie on www.themoviedb.org.\n\n   Effects:\n   - Navigates to the Full Cast & Crew page of the movie.\n   - Yields a scrapy.Request to parse the full credits page of the movie.\n\n   Args:\n      response: The response object containing the main page's HTML.\n\n   Yields:\n      scrapy.Request: A request to parse the full credits page of the movie.\n   \"\"\"\n   # Navigate to the Full Cast & Crew page\n   full_credits_url = response.url + '/cast/'\n   yield scrapy.Request(full_credits_url , callback=self.parse_full_credits)\n\nOn themoviedb webstite, once we are on our movie’s website (i.e. https://www.themoviedb.org/movie/157336-interstellar), if we click on the Full Cast and Crew button we are redirected to the cast page with the following URL: https://www.themoviedb.org/movie/157336-interstellar/cast. As we can see, the only difference between this URL and the previous is the added /cast at the end. Therefore, we will yeid the same URL as before and make sure to add the /cast/ the URL, so that we are now navigated to the Full Cast and Crew page."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#the-parse_full_credits-function",
    "href": "posts/Movie Web Scrapping/index.html#the-parse_full_credits-function",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "Next we will implement the parse_full_credits function of our spider class. The goal of this function will be to retrieve all the actor links for the actors in our movie. On the movie database website, each actor has their own page which lists information about thier bio, acting credits, and more. For example, the lead actor in Interstellar has the following page on TMDB: https://www.themoviedb.org/person/10297-matthew-mcconaughey. Here is the implementation of the parse_full_credits method:\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Parses the full credits page of a movie on www.themoviedb.org to retrieve actor links.\n\n        Assumptions:\n        - Assumes that the response is from the full credits page of a movie on www.themoviedb.org.\n\n        Effects:\n        - Extracts actor links from the page.\n        - Yields scrapy.Requests to parse each actor's page.\n\n        Args:\n            response: The response object containing the full credits page's HTML.\n\n        Yields:\n            scrapy.Request: Requests to parse each actor's page.\n        \"\"\"\n        # Extract actor links and yield requests for each actor's page\n        actor_links = response.css('div.info &gt; p &gt; a::attr(href)').getall()\n        for actor_link in actor_links:\n            yield scrapy.Request(get_scrapeops_url(\"https://www.themoviedb.org\" + actor_link), callback=self.parse_actor_page)\n\nIf we look at few of the pages on TMDB website, we notice that every acting page had the same starting URL: https://www.themoviedb.org. Therefore, we need to find what follows this URL, which we call actor_link, then add it to the starting URL, which will give the full URL for the acting page.\nBy going on the initial movie’s website and using the developer’s tools, I was able to find that link to every actor in the movie was listed in there. By following the path from div.info to p to a::attr(href), we are able to extract the second half the URL that we need.\nThus, we can yield the full URL for each actor in the movie."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#the-parse_actor_page-function",
    "href": "posts/Movie Web Scrapping/index.html#the-parse_actor_page-function",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "Finally, we need to implement our final function, the parse_actor_page function. The goal of this function to go through each of the actor’s page and extract all the movies and TV shows that they have appeared in. We will then yield the name of the actor and the list of movies and TV shows.\nLet’s first go over how to extract the name of the actor. The beggining of the parse_actor_page looks like this:\n\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Parses an actor's page on www.themoviedb.org to retrieve their name and movie/TV show credits.\n\n        Assumptions:\n        - Assumes that the response is from an actor's page on www.themoviedb.org.\n\n        Effects:\n        - Extracts the actor's name and their movie/TV show credits.\n        - Yields the actor's name and their credited movie/TV show.\n\n        Args:\n            response: The response object containing the actor's page's HTML.\n\n        Yields:\n            dict: A dictionary containing the actor's name and their credited movie/TV show.\n        \"\"\"\n\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\nUsing the developer’s tools again, I was able to see that every actor’s page had their name listed in the source code under, first under the head section, then under the title section. Therefore, the selector response.css(‘head &gt; title::text’).get(), would retrieve the title of the webpage. However, this response includes the title of the website. So for example, for Matthew McConaughey, the selector would give the following response: “Matthew McConaughey — The Movie Database (TMDB)”. Therefore we need to cut off everything from the long dash,‘—’, and onwards. By using the split function with ‘—’ and selecting the first half the split, [0], we are able to isolate the name of the actor. We will store this value in the variable actor_name.\nNext, we need to implement the second half the function that will extract the movie and TV names for every actor. Here is the full implementation of the parse_actor_page function, with the second half of the function:\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Parses an actor's page on www.themoviedb.org to retrieve their name and movie/TV show credits.\n\n        Assumptions:\n        - Assumes that the response is from an actor's page on www.themoviedb.org.\n\n        Effects:\n        - Extracts the actor's name and their movie/TV show credits.\n        - Yields the actor's name and their credited movie/TV show.\n\n        Args:\n            response: The response object containing the actor's page's HTML.\n\n        Yields:\n            dict: A dictionary containing the actor's name and their credited movie/TV show.\n        \"\"\"\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\n       # Find the index of 'Acting' in the list of credits\n        acting_index = response.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\n        # Select the element corresponding to 'Acting' and its associated table\n        acting_only = response.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\n        # Extract movie or TV names from the selected table\n        movie_or_TV_name = acting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\n\n        for movie_or_TV_name in movie_or_TV_name:\n            yield {\n                \"actor\":actor_name,\n                \"movie_or_TV_name\": movie_or_TV_name\n            }\n\nThis part was a bit tricky as we want to make sure that we only select movies and TV shows in which the person’s page that we are personing were credited in an acting role, as some also have production and other credits that we do not want included in the list.\nIn order to do this we must take advantage of the indexing in source code that will allow us to select only the acting roles. After looking an the source code on the page, we can see that all the acting roles for an actor had a line in common before listing all acting roles:\n&lt;h3 class=“zero”&gt;Acting&lt;&gt;\n\nTherefore we can use the follwoing xpath response to find the index for each actor:\n\nresponse.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\nWe will store this index under the variable name actor_index.\nNext we must use this variable to find the table on the actors page that corresponds to the acting index. Thus we can use the following to return this section of the table under the acting index.\n\nresponse.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\nThe reason we must add the +1 to acting_index is that xpath starts its index at 1 instead of 0. Therefore we must account for this change by adding the one.\nThe final stage of our extraction is to go through the table we selected and retrieve the title of each movie or TV show that the actor appears in.\nUsing the inspect element tool again, we can see that information is located under the &lt;a class = “tooltip”&gt; section, with the text of the film being wrapped by the &lt;bdi&gt; markers.\nTherefore, we can use the followig line to get all the titles of the films and shows, as we are already under the acting table of the page. Therefore movie_or_TV_name will be a list of the credits for each actor.\n\nacting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\nFinally, we yield the actors name and the credited film or TV show under the names “actor” and “movie_or_TV_show”, so when we export to a CSV file they will be the name of our columns.\nNow that we have completed our spider class, we can run our code and output to a CSV file called output. Therefore we must run the following line in our terminal.\nscrapy crawl tmdb_spider -o output.csv -a subdir=157336-interstellar\nThis will create an output called output.csv, we can read this file into a Pandas dataframe to view our data."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#first-data-analysis-number-of-shared-actors",
    "href": "posts/Movie Web Scrapping/index.html#first-data-analysis-number-of-shared-actors",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "Now that we have our output we can convert the csv file into a Pandas dataframe to view and analyze our data. In a seperate file we can import Pandas and run the following line of code:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"output.csv\")\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nAs we can see we have 2063 rows of movie and TV show titles, with the columns actor and movie_or_TV_name.\nHere is a what our data table would look like:\n\ndf.head(10)\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nMatthew McConaughey\n2024\n\n\n1\nMatthew McConaughey\nThe Lost Bus\n\n\n2\nMatthew McConaughey\nThe Rivals of Amziah King\n\n\n3\nMatthew McConaughey\nAgent Elvis\n\n\n4\nMatthew McConaughey\nThe Jennifer Hudson Show\n\n\n5\nMatthew McConaughey\nDeep in the Heart: A Texas Wildlife Story\n\n\n6\nMatthew McConaughey\nSing 2\n\n\n7\nMatthew McConaughey\nCome Home\n\n\n8\nMatthew McConaughey\n05\n\n\n9\nMatthew McConaughey\nRoll Up Your Sleeves\n\n\n\n\n\n\n\nWe can see that Matthew McConaughey is the first actor in our list and we can see his associated roles, for example “The Lost Bus” and “Come Home” are a couple of his roles.\nNext we can see compute a sorted list with the top movies and TV shows that share actors from the movie Interstellar.\nFirst we need to count the data by the tile of the role:\n\ncounted = df[\"movie_or_TV_name\"].value_counts()\ncounted\n\nmovie_or_TV_name\nInterstellar                   35\nSaturday Night Live            10\nThe View                        9\nLate Night with Seth Meyers     9\nInside 'Interstellar'           8\n                               ..\nMayo                            1\nAs You Like It                  1\nBorn Equal                      1\nFive Days                       1\nThree Below Zero                1\nName: count, Length: 1754, dtype: int64\n\n\nAs we can see the movie that we based our web crawler on appears first, so we should drop that from the list.\n\ncounted = counted.drop(counted.index[0])\n\nNext, in order to for us to work with this data easier we must convert the Pandas series into a Pandas Data Frame and reset the index.\n\ncounted = counted.to_frame()\ncounted.reset_index(inplace=True)\n\nWe will also renmae the columns to Movie or Show Title and Number of Shared Actors.\n\ncounted = counted.rename(columns={\"movie_or_TV_name\":\"Movie or Show Title\", \"count\":\"Number of Shared Actors\"})\n\nThus our new dataframe looks like the following:\n\ncounted\n\n\n\n\n\n\n\n\nMovie or Show Title\nNumber of Shared Actors\n\n\n\n\n0\nSaturday Night Live\n10\n\n\n1\nThe View\n9\n\n\n2\nLate Night with Seth Meyers\n9\n\n\n3\nInside 'Interstellar'\n8\n\n\n4\nThe Tonight Show Starring Jimmy Fallon\n8\n\n\n...\n...\n...\n\n\n1748\nMayo\n1\n\n\n1749\nAs You Like It\n1\n\n\n1750\nBorn Equal\n1\n\n\n1751\nFive Days\n1\n\n\n1752\nThree Below Zero\n1\n\n\n\n\n1753 rows × 2 columns\n\n\n\nNow we can use plotly to create a bar graph of the some of the most common shared movies or TV shows among the actors in Interstellar. Since there are 1753 movies or TV shows, we will only pick the top 15 titles to display.\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nimport plotly.express as px\n\ntop_counted = counted.head(15)\n\n# Create a bar chart using Plotly\nfig = px.bar(top_counted, x='Movie or Show Title', y='Number of Shared Actors', title='Number of Shared Actors per Movie or Show')\n\n# Show the plot\nfig.show()\n\n\n\n\nAs we can see, the top shared movies and shows are displayed in an interactive bar graph format, where if we hover over the bar we can exactly how many shared actors there are. The results are somewhat expected as well. We can see a majority of the shared titles are from late night shows or talk shows. This makes sense, as actors and celebrities often appear on these shows. Since Interstellar has a good amount of famous actors that would appear on these shows, the top results are something that we expect.\nOther top shared roles are Inside “Interstellar”, which is a documentary on how the film was made, so it would make sense that a lot of the actors from Interstellar would appear in the documentary.\nAnother film of note is The Dark Knight Rises. This film, along with Interstellar, was directed by Christopher Nolan, and he is know for using some of the same actors in his films. Therefore it is not suprising to see one of his films on this list."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#second-data-analysis-number-of-roles-per-actor",
    "href": "posts/Movie Web Scrapping/index.html#second-data-analysis-number-of-roles-per-actor",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "For our second data analysis, we can find the number of roles that each actor has been in and then display a bar graph to visualize the data.\nWe will count the number of roles and then sort by highest of number of roles. We will use plotly again to produce the graph.\n\n# Count number of roles and sort by highest number\nroles_count = df.groupby('actor').size().reset_index(name='Number of Roles')\n\nroles_count = roles_count.sort_values(by='Number of Roles', ascending=False)\n\n# Create a bar chart using Plotly\nfig = px.bar(roles_count, x='actor', y='Number of Roles', title='Number of Roles for Each Actor')\n\n# Show the plot\nfig.show()\n\n\n\n\nWe can see that the interactive bar graph is displayed. The results are also what we expected. The actor with (by far) the highest number of roles is Michael Caine, with 224 roles. This is not suprising as Michael Caine is one of the oldest actors and has had a very long career in the film industry.\nWe can also see Matt Damon, Matthew McConaughey, and Anne Hathaway have the fourth, fifth, and sixth top roles. These are famous contemporary actors who have appeared in many famous films as well as have had very successfull careers. Therefore it is not suprising to see them near the top of the list."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#conclusion",
    "href": "posts/Movie Web Scrapping/index.html#conclusion",
    "title": "Web Scrapping with scrapy",
    "section": "",
    "text": "Thank you for reading my blog post! I hope that you were able to learn more about web scrapping as well as how to display scrapped data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arvin Hosseini’s Blog",
    "section": "",
    "text": "Image Classification of Cats and Dogs\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nSimulation of Heat Diffusion\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nWeb Development with Flask\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nWeb Scrapping with scrapy\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization and Databases\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization Tutorial\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\nNo matching items"
  }
]