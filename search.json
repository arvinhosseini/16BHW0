[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html",
    "href": "posts/Movie Web Scrapping/index.html",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Hello! In this blog post I will go over how to create a web scrapper using scrapy. For the purpose of this post, the goal of our web scrapper is to find all actors from a particular movie and output the list the actors along with all other movies or TV shows they have been in. The website we will be webscrapping is www.themoviedb.org, which is a website that holds movie and TV show information.\n\n\nThe first step is to include the neccesary packages. When I was creating this webscrapper, I had issues with 403 errors. Therefore, I used a proxy via ScrapeOps to help resolve the errors and successfully scrape the websites.\n\nimport scrapy\nfrom urllib.parse import urlencode\n\nAPI_KEY = \"816aed8f-f731-424c-b089-7c03af87389e\"\n\n\ndef get_scrapeops_url(url):\n    payload = {'api_key': API_KEY, 'url': url}\n    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n    return proxy_url\n\nThe get_scrapeops_url function will serve as a proxy and we can input a normal url into the function and it allow for us to not get flagged.\n\n\n\nNow that we have inlcuded everything neccessary, we can start building our Spider class from scrapy. First we need to declare the class and the definite the constructor function init which would define the starting url that we are trying to scrape.\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [get_scrapeops_url(f\"https://www.themoviedb.org/movie/{subdir}/\")]\n\nAs we can see, the we created the TmbdSpider class and named it ‘tmbd_spider’ and used the proxy to build our constructor. The {subdir} portion of the URL is what movie we are trying to scrape. For example, my favorie moive is Interstellar, so subdir will be defined as 157336-interstellar, since the full URL is https://www.themoviedb.org/movie/157336-interstellar.\nNext, we need to define our parse function, which will take our spider web scrapper to the cast and crew section of our movie.\n\ndef parse(self, response):\n   # Navigate to the Full Cast & Crew page\n   full_credits_url = response.url + '/cast/'\n   yield scrapy.Request(full_credits_url , callback=self.parse_full_credits)\n\nOn themoviedb webstite, once we are on our movie’s website (i.e. https://www.themoviedb.org/movie/157336-interstellar), if we click on the Full Cast and Crew button we are redirected to the cast page with the following URL: https://www.themoviedb.org/movie/157336-interstellar/cast. As we can see, the only difference between this URL and the previous is the added /cast at the end. Therefore, we will yeid the same URL as before and make sure to add the /cast/ the URL, so that we are now navigated to the Full Cast and Crew page.\n\n\n\nNext we will implement the parse_full_credits function of our spider class. The goal of this function will be to retrieve all the actor links for the actors in our movie. On the movie database website, each actor has their own page which lists information about thier bio, acting credits, and more. For example, the lead actor in Interstellar has the following page on TMDB: https://www.themoviedb.org/person/10297-matthew-mcconaughey. Here is the implementation of the parse_full_credits method:\n\ndef parse_full_credits(self, response):\n        # Extract actor links and yield requests for each actor's page\n        actor_links = response.css('div.info &gt; p &gt; a::attr(href)').getall()\n\n        for actor_link in actor_links:\n            yield scrapy.Request(get_scrapeops_url(\"https://www.themoviedb.org\" + actor_link), callback=self.parse_actor_page)\n\nIf we look at few of the pages on TMDB website, we notice that every acting page had the same starting URL: https://www.themoviedb.org. Therefore, we need to find what follows this URL, which we call actor_link, then add it to the starting URL, which will give the full URL for the acting page.\nBy going on the initial movie’s website and using the developer’s tools, I was able to find that link to every actor in the movie was listed in there. By following the path from div.info to p to a::attr(href), we are able to extract the second half the URL that we need.\nThus, we can yield the full URL for each actor in the movie.\n\n\n\nFinally, we need to implement our final function, the parse_actor_page function. The goal of this function to go through each of the actor’s page and extract all the movies and TV shows that they have appeared in. We will then yield the name of the actor and the list of movies and TV shows.\nLet’s first go over how to extract the name of the actor. The beggining of the parse_actor_page looks like this:\n\ndef parse_actor_page(self, response):\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\nUsing the developer’s tools again, I was able to see that every actor’s page had their name listed in the source code under, first under the head section, then under the title section. Therefore, the selector response.css(‘head &gt; title::text’).get(), would retrieve the title of the webpage. However, this response includes the title of the website. So for example, for Matthew McConaughey, the selector would give the following response: “Matthew McConaughey — The Movie Database (TMDB)”. Therefore we need to cut off everything from the long dash,‘—’, and onwards. By using the split function with ‘—’ and selecting the first half the split, [0], we are able to isolate the name of the actor. We will store this value in the variable actor_name.\nNext, we need to implement the second half the function that will extract the movie and TV names for every actor. Here is the full implementation of the parse_actor_page function, with the second half of the function:\n\ndef parse_actor_page(self, response):\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\n       # Find the index of 'Acting' in the list of credits\n        acting_index = response.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\n        # Select the element corresponding to 'Acting' and its associated table\n        acting_only = response.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\n        # Extract movie or TV names from the selected table\n        movie_or_TV_name = acting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\n\n        for movie_or_TV_name in movie_or_TV_name:\n            yield {\n                \"actor\":actor_name,\n                \"movie_or_TV_name\": movie_or_TV_name\n            }\n\nThis part was a bit tricky as we want to make sure that we only select movies and TV shows in which the person’s page that we are personing were credited in an acting role, as some also have production and other credits that we do not want included in the list.\nIn order to do this we must take advantage of the indexing in source code that will allow us to select only the acting roles. After looking an the source code on the page, we can see that all the acting roles for an actor had a line in common before listing all acting roles:\n&lt;h3 class=“zero”&gt;Acting&lt;&gt;\n\nTherefore we can use the follwoing xpath response to find the index for each actor:\n\nresponse.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\nWe will store this index under the variable name actor_index.\nNext we must use this variable to find the table on the actors page that corresponds to the acting index. Thus we can use the following to return this section of the table under the acting index.\n\nresponse.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\nThe reason we must add the +1 to acting_index is that xpath starts its index at 1 instead of 0. Therefore we must account for this change by adding the one.\nThe final stage of our extraction is to go through the table we selected and retrieve the title of each movie or TV show that the actor appears in.\nUsing the inspect element tool again, we can see that information is located under the &lt;a class = “tooltip”&gt; section, with the text of the film being wrapped by the &lt;bdi&gt; markers.\nTherefore, we can use the followig line to get all the titles of the films and shows, as we are already under the acting table of the page. Therefore movie_or_TV_name will be a list of the credits for each actor.\n\nacting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\nFinally, we yield the actors name and the credited film or TV show under the names “actor” and “movie_or_TV_show”, so when we export to a CSV file they will be the name of our columns.\nNow that we have completed our spider class, we can run our code and output to a CSV file called output. Therefore we must run the following line in our terminal.\nscrapy crawl tmdb_spider -o output.csv -a subdir=157336-interstellar\nThis will create an output called output.csv, we can read this file into a Pandas dataframe to view our data.\n\n\n\nNow that we have our output we can convert the csv file into a Pandas dataframe to view and analyze our data. In a seperate file we can import Pandas and run the following line of code:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"output.csv\")\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nAs we can see we have 2063 rows of movie and TV show titles, with the columns actor and movie_or_TV_name.\nHere is a what our data table would look like:\n\ndf.head(10)\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nMatthew McConaughey\n2024\n\n\n1\nMatthew McConaughey\nThe Lost Bus\n\n\n2\nMatthew McConaughey\nThe Rivals of Amziah King\n\n\n3\nMatthew McConaughey\nAgent Elvis\n\n\n4\nMatthew McConaughey\nThe Jennifer Hudson Show\n\n\n5\nMatthew McConaughey\nDeep in the Heart: A Texas Wildlife Story\n\n\n6\nMatthew McConaughey\nSing 2\n\n\n7\nMatthew McConaughey\nCome Home\n\n\n8\nMatthew McConaughey\n05\n\n\n9\nMatthew McConaughey\nRoll Up Your Sleeves\n\n\n\n\n\n\n\nWe can see that Matthew McConaughey is the first actor in our list and we can see his associated roles, for example “The Lost Bus” and “Come Home” are a couple of his roles.\nNext we can see compute a sorted list with the top movies and TV shows that share actors from the movie Interstellar.\nFirst we need to count the data by the tile of the role:\n\ncounted = df[\"movie_or_TV_name\"].value_counts()\ncounted\n\nmovie_or_TV_name\nInterstellar                   35\nSaturday Night Live            10\nThe View                        9\nLate Night with Seth Meyers     9\nInside 'Interstellar'           8\n                               ..\nMayo                            1\nAs You Like It                  1\nBorn Equal                      1\nFive Days                       1\nThree Below Zero                1\nName: count, Length: 1754, dtype: int64\n\n\nAs we can see the movie that we based our web crawler on appears first, so we should drop that from the list.\n\ncounted = counted.drop(counted.index[0])\n\nNext, in order to for us to work with this data easier we must convert the Pandas series into a Pandas Data Frame and reset the index.\n\ncounted = counted.to_frame()\ncounted.reset_index(inplace=True)\n\nWe will also renmae the columns to Movie or Show Title and Number of Shared Actors.\n\ncounted = counted.rename(columns={\"movie_or_TV_name\":\"Movie or Show Title\", \"count\":\"Number of Shared Actors\"})\n\nThus our new dataframe looks like the following:\n\ncounted\n\n\n\n\n\n\n\n\nMovie or Show Title\nNumber of Shared Actors\n\n\n\n\n0\nSaturday Night Live\n10\n\n\n1\nThe View\n9\n\n\n2\nLate Night with Seth Meyers\n9\n\n\n3\nInside 'Interstellar'\n8\n\n\n4\nThe Tonight Show Starring Jimmy Fallon\n8\n\n\n...\n...\n...\n\n\n1748\nMayo\n1\n\n\n1749\nAs You Like It\n1\n\n\n1750\nBorn Equal\n1\n\n\n1751\nFive Days\n1\n\n\n1752\nThree Below Zero\n1\n\n\n\n\n1753 rows × 2 columns\n\n\n\nNow we can use plotly to create a bar graph of the some of the most common shared movies or TV shows among the actors in Interstellar. Since there are 1753 movies or TV shows, we will only pick the top 15 titles to display.\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nimport plotly.express as px\n\ntop_counted = counted.head(15)\n\n# Create a bar chart using Plotly\nfig = px.bar(top_counted, x='Movie or Show Title', y='Number of Shared Actors', title='Number of Shared Actors per Movie or Show')\n\n# Show the plot\nfig.show()\n\n\n\n\nAs we can see, the top shared movies and shows are displayed in an interactive bar graph format, where if we hover over the bar we can exactly how many shared actors there are. The results are somewhat expected as well. We can see a majority of the shared titles are from late night shows or talk shows. This makes sense, as actors and celebrities often appear on these shows. Since Interstellar has a good amount of famous actors that would appear on these shows, the top results are something that we expect.\nOther top shared roles are Inside “Interstellar”, which is a documentary on how the film was made, so it would make sense that a lot of the actors from Interstellar would appear in the documentary.\nAnother film of note is The Dark Knight Rises. This film, along with Interstellar, was directed by Christopher Nolan, and he is know for using some of the same actors in his films. Therefore it is not suprising to see one of his films on this list.\n\n\n\nFor our second data analysis, we can find the number of roles that each actor has been in and then display a bar graph to visualize the data.\nWe will count the number of roles and then sort by highest of number of roles. We will use plotly again to produce the graph.\n\n# Count number of roles and sort by highest number\nroles_count = df.groupby('actor').size().reset_index(name='Number of Roles')\n\nroles_count = roles_count.sort_values(by='Number of Roles', ascending=False)\n\n# Create a bar chart using Plotly\nfig = px.bar(roles_count, x='actor', y='Number of Roles', title='Number of Roles for Each Actor')\n\n# Show the plot\nfig.show()\n\n\n\n\nWe can see that the interactive bar graph is displayed. The results are also what we expected. The actor with (by far) the highest number of roles is Michael Caine, with 224 roles. This is not suprising as Michael Caine is one of the oldest actors and has had a very long career in the film industry.\nWe can also see Matt Damon, Matthew McConaughey, and Anne Hathaway have the fourth, fifth, and sixth top roles. These are famous contemporary actors who have appeared in many famous films as well as have had very successfull careers. Therefore it is not suprising to see them near the top of the list.\n\n\n\nThank you for reading my blog post! I hope that you were able to learn more about web scrapping as well as how to display scrapped data."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#set-up",
    "href": "posts/Movie Web Scrapping/index.html#set-up",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "The first step is to include the neccesary packages. When I was creating this webscrapper, I had issues with 403 errors. Therefore, I used a proxy via ScrapeOps to help resolve the errors and successfully scrape the websites.\n\nimport scrapy\nfrom urllib.parse import urlencode\n\nAPI_KEY = \"816aed8f-f731-424c-b089-7c03af87389e\"\n\n\ndef get_scrapeops_url(url):\n    payload = {'api_key': API_KEY, 'url': url}\n    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n    return proxy_url\n\nThe get_scrapeops_url function will serve as a proxy and we can input a normal url into the function and it allow for us to not get flagged."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#declaring-class-and-defining-parse-function",
    "href": "posts/Movie Web Scrapping/index.html#declaring-class-and-defining-parse-function",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Now that we have inlcuded everything neccessary, we can start building our Spider class from scrapy. First we need to declare the class and the definite the constructor function init which would define the starting url that we are trying to scrape.\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [get_scrapeops_url(f\"https://www.themoviedb.org/movie/{subdir}/\")]\n\nAs we can see, the we created the TmbdSpider class and named it ‘tmbd_spider’ and used the proxy to build our constructor. The {subdir} portion of the URL is what movie we are trying to scrape. For example, my favorie moive is Interstellar, so subdir will be defined as 157336-interstellar, since the full URL is https://www.themoviedb.org/movie/157336-interstellar.\nNext, we need to define our parse function, which will take our spider web scrapper to the cast and crew section of our movie.\n\ndef parse(self, response):\n   # Navigate to the Full Cast & Crew page\n   full_credits_url = response.url + '/cast/'\n   yield scrapy.Request(full_credits_url , callback=self.parse_full_credits)\n\nOn themoviedb webstite, once we are on our movie’s website (i.e. https://www.themoviedb.org/movie/157336-interstellar), if we click on the Full Cast and Crew button we are redirected to the cast page with the following URL: https://www.themoviedb.org/movie/157336-interstellar/cast. As we can see, the only difference between this URL and the previous is the added /cast at the end. Therefore, we will yeid the same URL as before and make sure to add the /cast/ the URL, so that we are now navigated to the Full Cast and Crew page."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#the-parse_full_credits-function",
    "href": "posts/Movie Web Scrapping/index.html#the-parse_full_credits-function",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Next we will implement the parse_full_credits function of our spider class. The goal of this function will be to retrieve all the actor links for the actors in our movie. On the movie database website, each actor has their own page which lists information about thier bio, acting credits, and more. For example, the lead actor in Interstellar has the following page on TMDB: https://www.themoviedb.org/person/10297-matthew-mcconaughey. Here is the implementation of the parse_full_credits method:\n\ndef parse_full_credits(self, response):\n        # Extract actor links and yield requests for each actor's page\n        actor_links = response.css('div.info &gt; p &gt; a::attr(href)').getall()\n\n        for actor_link in actor_links:\n            yield scrapy.Request(get_scrapeops_url(\"https://www.themoviedb.org\" + actor_link), callback=self.parse_actor_page)\n\nIf we look at few of the pages on TMDB website, we notice that every acting page had the same starting URL: https://www.themoviedb.org. Therefore, we need to find what follows this URL, which we call actor_link, then add it to the starting URL, which will give the full URL for the acting page.\nBy going on the initial movie’s website and using the developer’s tools, I was able to find that link to every actor in the movie was listed in there. By following the path from div.info to p to a::attr(href), we are able to extract the second half the URL that we need.\nThus, we can yield the full URL for each actor in the movie."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#the-parse_actor_page-function",
    "href": "posts/Movie Web Scrapping/index.html#the-parse_actor_page-function",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Finally, we need to implement our final function, the parse_actor_page function. The goal of this function to go through each of the actor’s page and extract all the movies and TV shows that they have appeared in. We will then yield the name of the actor and the list of movies and TV shows.\nLet’s first go over how to extract the name of the actor. The beggining of the parse_actor_page looks like this:\n\ndef parse_actor_page(self, response):\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\nUsing the developer’s tools again, I was able to see that every actor’s page had their name listed in the source code under, first under the head section, then under the title section. Therefore, the selector response.css(‘head &gt; title::text’).get(), would retrieve the title of the webpage. However, this response includes the title of the website. So for example, for Matthew McConaughey, the selector would give the following response: “Matthew McConaughey — The Movie Database (TMDB)”. Therefore we need to cut off everything from the long dash,‘—’, and onwards. By using the split function with ‘—’ and selecting the first half the split, [0], we are able to isolate the name of the actor. We will store this value in the variable actor_name.\nNext, we need to implement the second half the function that will extract the movie and TV names for every actor. Here is the full implementation of the parse_actor_page function, with the second half of the function:\n\ndef parse_actor_page(self, response):\n        # Extract actor name and movies/TV shows they have worked in\n        actor_name = response.css('head &gt; title::text').get()\n\n        # Extract only the part before the hyphen\n        actor_name = actor_name.split('—')[0].strip()\n\n       # Find the index of 'Acting' in the list of credits\n        acting_index = response.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\n        # Select the element corresponding to 'Acting' and its associated table\n        acting_only = response.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\n        # Extract movie or TV names from the selected table\n        movie_or_TV_name = acting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\n\n        for movie_or_TV_name in movie_or_TV_name:\n            yield {\n                \"actor\":actor_name,\n                \"movie_or_TV_name\": movie_or_TV_name\n            }\n\nThis part was a bit tricky as we want to make sure that we only select movies and TV shows in which the person’s page that we are personing were credited in an acting role, as some also have production and other credits that we do not want included in the list.\nIn order to do this we must take advantage of the indexing in source code that will allow us to select only the acting roles. After looking an the source code on the page, we can see that all the acting roles for an actor had a line in common before listing all acting roles:\n&lt;h3 class=“zero”&gt;Acting&lt;&gt;\n\nTherefore we can use the follwoing xpath response to find the index for each actor:\n\nresponse.xpath('//div[@class=\"credits_list\"]/h3/text()').getall().index('Acting')\n\nWe will store this index under the variable name actor_index.\nNext we must use this variable to find the table on the actors page that corresponds to the acting index. Thus we can use the following to return this section of the table under the acting index.\n\nresponse.xpath(f'(//div[@class=\"credits_list\"]/table[@class=\"card credits\"])[{acting_index + 1}]')\n\nThe reason we must add the +1 to acting_index is that xpath starts its index at 1 instead of 0. Therefore we must account for this change by adding the one.\nThe final stage of our extraction is to go through the table we selected and retrieve the title of each movie or TV show that the actor appears in.\nUsing the inspect element tool again, we can see that information is located under the &lt;a class = “tooltip”&gt; section, with the text of the film being wrapped by the &lt;bdi&gt; markers.\nTherefore, we can use the followig line to get all the titles of the films and shows, as we are already under the acting table of the page. Therefore movie_or_TV_name will be a list of the credits for each actor.\n\nacting_only.xpath('.//a[@class=\"tooltip\"]/bdi/text()').getall()\n\nFinally, we yield the actors name and the credited film or TV show under the names “actor” and “movie_or_TV_show”, so when we export to a CSV file they will be the name of our columns.\nNow that we have completed our spider class, we can run our code and output to a CSV file called output. Therefore we must run the following line in our terminal.\nscrapy crawl tmdb_spider -o output.csv -a subdir=157336-interstellar\nThis will create an output called output.csv, we can read this file into a Pandas dataframe to view our data."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#first-data-analysis-number-of-shared-actors",
    "href": "posts/Movie Web Scrapping/index.html#first-data-analysis-number-of-shared-actors",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Now that we have our output we can convert the csv file into a Pandas dataframe to view and analyze our data. In a seperate file we can import Pandas and run the following line of code:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"output.csv\")\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2063 entries, 0 to 2062\nData columns (total 2 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   actor             2063 non-null   object\n 1   movie_or_TV_name  2063 non-null   object\ndtypes: object(2)\nmemory usage: 32.4+ KB\n\n\nAs we can see we have 2063 rows of movie and TV show titles, with the columns actor and movie_or_TV_name.\nHere is a what our data table would look like:\n\ndf.head(10)\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nMatthew McConaughey\n2024\n\n\n1\nMatthew McConaughey\nThe Lost Bus\n\n\n2\nMatthew McConaughey\nThe Rivals of Amziah King\n\n\n3\nMatthew McConaughey\nAgent Elvis\n\n\n4\nMatthew McConaughey\nThe Jennifer Hudson Show\n\n\n5\nMatthew McConaughey\nDeep in the Heart: A Texas Wildlife Story\n\n\n6\nMatthew McConaughey\nSing 2\n\n\n7\nMatthew McConaughey\nCome Home\n\n\n8\nMatthew McConaughey\n05\n\n\n9\nMatthew McConaughey\nRoll Up Your Sleeves\n\n\n\n\n\n\n\nWe can see that Matthew McConaughey is the first actor in our list and we can see his associated roles, for example “The Lost Bus” and “Come Home” are a couple of his roles.\nNext we can see compute a sorted list with the top movies and TV shows that share actors from the movie Interstellar.\nFirst we need to count the data by the tile of the role:\n\ncounted = df[\"movie_or_TV_name\"].value_counts()\ncounted\n\nmovie_or_TV_name\nInterstellar                   35\nSaturday Night Live            10\nThe View                        9\nLate Night with Seth Meyers     9\nInside 'Interstellar'           8\n                               ..\nMayo                            1\nAs You Like It                  1\nBorn Equal                      1\nFive Days                       1\nThree Below Zero                1\nName: count, Length: 1754, dtype: int64\n\n\nAs we can see the movie that we based our web crawler on appears first, so we should drop that from the list.\n\ncounted = counted.drop(counted.index[0])\n\nNext, in order to for us to work with this data easier we must convert the Pandas series into a Pandas Data Frame and reset the index.\n\ncounted = counted.to_frame()\ncounted.reset_index(inplace=True)\n\nWe will also renmae the columns to Movie or Show Title and Number of Shared Actors.\n\ncounted = counted.rename(columns={\"movie_or_TV_name\":\"Movie or Show Title\", \"count\":\"Number of Shared Actors\"})\n\nThus our new dataframe looks like the following:\n\ncounted\n\n\n\n\n\n\n\n\nMovie or Show Title\nNumber of Shared Actors\n\n\n\n\n0\nSaturday Night Live\n10\n\n\n1\nThe View\n9\n\n\n2\nLate Night with Seth Meyers\n9\n\n\n3\nInside 'Interstellar'\n8\n\n\n4\nThe Tonight Show Starring Jimmy Fallon\n8\n\n\n...\n...\n...\n\n\n1748\nMayo\n1\n\n\n1749\nAs You Like It\n1\n\n\n1750\nBorn Equal\n1\n\n\n1751\nFive Days\n1\n\n\n1752\nThree Below Zero\n1\n\n\n\n\n1753 rows × 2 columns\n\n\n\nNow we can use plotly to create a bar graph of the some of the most common shared movies or TV shows among the actors in Interstellar. Since there are 1753 movies or TV shows, we will only pick the top 15 titles to display.\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nimport plotly.express as px\n\ntop_counted = counted.head(15)\n\n# Create a bar chart using Plotly\nfig = px.bar(top_counted, x='Movie or Show Title', y='Number of Shared Actors', title='Number of Shared Actors per Movie or Show')\n\n# Show the plot\nfig.show()\n\n\n\n\nAs we can see, the top shared movies and shows are displayed in an interactive bar graph format, where if we hover over the bar we can exactly how many shared actors there are. The results are somewhat expected as well. We can see a majority of the shared titles are from late night shows or talk shows. This makes sense, as actors and celebrities often appear on these shows. Since Interstellar has a good amount of famous actors that would appear on these shows, the top results are something that we expect.\nOther top shared roles are Inside “Interstellar”, which is a documentary on how the film was made, so it would make sense that a lot of the actors from Interstellar would appear in the documentary.\nAnother film of note is The Dark Knight Rises. This film, along with Interstellar, was directed by Christopher Nolan, and he is know for using some of the same actors in his films. Therefore it is not suprising to see one of his films on this list."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#second-data-analysis-number-of-roles-per-actor",
    "href": "posts/Movie Web Scrapping/index.html#second-data-analysis-number-of-roles-per-actor",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "For our second data analysis, we can find the number of roles that each actor has been in and then display a bar graph to visualize the data.\nWe will count the number of roles and then sort by highest of number of roles. We will use plotly again to produce the graph.\n\n# Count number of roles and sort by highest number\nroles_count = df.groupby('actor').size().reset_index(name='Number of Roles')\n\nroles_count = roles_count.sort_values(by='Number of Roles', ascending=False)\n\n# Create a bar chart using Plotly\nfig = px.bar(roles_count, x='actor', y='Number of Roles', title='Number of Roles for Each Actor')\n\n# Show the plot\nfig.show()\n\n\n\n\nWe can see that the interactive bar graph is displayed. The results are also what we expected. The actor with (by far) the highest number of roles is Michael Caine, with 224 roles. This is not suprising as Michael Caine is one of the oldest actors and has had a very long career in the film industry.\nWe can also see Matt Damon, Matthew McConaughey, and Anne Hathaway have the fourth, fifth, and sixth top roles. These are famous contemporary actors who have appeared in many famous films as well as have had very successfull careers. Therefore it is not suprising to see them near the top of the list."
  },
  {
    "objectID": "posts/Movie Web Scrapping/index.html#conclusion",
    "href": "posts/Movie Web Scrapping/index.html#conclusion",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Thank you for reading my blog post! I hope that you were able to learn more about web scrapping as well as how to display scrapped data."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello! Here is my welcome post for my new blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Tutorial/index.html",
    "href": "posts/Tutorial/index.html",
    "title": "Data Visualization Tutorial",
    "section": "",
    "text": "Hello! Welcome to my Data Visualization Tutorial. For the tutorial, I will be using the Palmer Penguins data set. The url for this data set can be found below.\nThe first step is to import pandas and load in the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Tutorial/index.html#intro-and-set-up",
    "href": "posts/Tutorial/index.html#intro-and-set-up",
    "title": "Data Visualization Tutorial",
    "section": "",
    "text": "Hello! Welcome to my Data Visualization Tutorial. For the tutorial, I will be using the Palmer Penguins data set. The url for this data set can be found below.\nThe first step is to import pandas and load in the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)"
  },
  {
    "objectID": "posts/Tutorial/index.html#using-head-and-info-functions",
    "href": "posts/Tutorial/index.html#using-head-and-info-functions",
    "title": "Data Visualization Tutorial",
    "section": "Using Head() and Info() Functions",
    "text": "Using Head() and Info() Functions\nNext, we can look at what the data looks like using the head and describe function in pandas.\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   studyName            344 non-null    object \n 1   Sample Number        344 non-null    int64  \n 2   Species              344 non-null    object \n 3   Region               344 non-null    object \n 4   Island               344 non-null    object \n 5   Stage                344 non-null    object \n 6   Individual ID        344 non-null    object \n 7   Clutch Completion    344 non-null    object \n 8   Date Egg             344 non-null    object \n 9   Culmen Length (mm)   342 non-null    float64\n 10  Culmen Depth (mm)    342 non-null    float64\n 11  Flipper Length (mm)  342 non-null    float64\n 12  Body Mass (g)        342 non-null    float64\n 13  Sex                  334 non-null    object \n 14  Delta 15 N (o/oo)    330 non-null    float64\n 15  Delta 13 C (o/oo)    331 non-null    float64\n 16  Comments             26 non-null     object \ndtypes: float64(6), int64(1), object(10)\nmemory usage: 45.8+ KB\n\n\nAs we can see from the displayed info, there are 344 entries in the dataframe as well as 17 different columns that are associated with each entry. So each data point has a studyName, Sample Number, ect. We can see that some columns, such as Comments or Body Mass, have a non-Null count of under 344. This means that some data entries in the dataframe do not have any values for some of these columns.\nFor this tuturial, lets look at the following four columns :‘culmen length’, ‘culmen depth’, ‘flipper length’, ‘body mass’ and see how they compare based on the study. Since some of these are null, lets remove the rows that have a null value for at least one of these columns.\n\ncolumns_of_interest = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\ndf_cleaned = penguins.dropna(subset=columns_of_interest)\n\n\ndf_cleaned.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 342 entries, 0 to 343\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   studyName            342 non-null    object \n 1   Sample Number        342 non-null    int64  \n 2   Species              342 non-null    object \n 3   Region               342 non-null    object \n 4   Island               342 non-null    object \n 5   Stage                342 non-null    object \n 6   Individual ID        342 non-null    object \n 7   Clutch Completion    342 non-null    object \n 8   Date Egg             342 non-null    object \n 9   Culmen Length (mm)   342 non-null    float64\n 10  Culmen Depth (mm)    342 non-null    float64\n 11  Flipper Length (mm)  342 non-null    float64\n 12  Body Mass (g)        342 non-null    float64\n 13  Sex                  334 non-null    object \n 14  Delta 15 N (o/oo)    330 non-null    float64\n 15  Delta 13 C (o/oo)    331 non-null    float64\n 16  Comments             25 non-null     object \ndtypes: float64(6), int64(1), object(10)\nmemory usage: 48.1+ KB\n\n\nWe can see that two rows got dropped from the dataframe, as the total number of entries is now 342."
  },
  {
    "objectID": "posts/Tutorial/index.html#visualization-using-box-plot",
    "href": "posts/Tutorial/index.html#visualization-using-box-plot",
    "title": "Data Visualization Tutorial",
    "section": "Visualization using Box Plot",
    "text": "Visualization using Box Plot\nNext, we can use matplotlib to visualize some of the data, using these four columns that we are intersted in.\n\n#import matplotlib\nimport matplotlib.pyplot as plt\n\nNow we can write a basic for loop that displays the box plot for each of these four columns.\n\nfor column in columns_of_interest:\n    plt.figure(figsize=(8, 5))\n    df_cleaned[column].plot(kind='box', vert=False)\n    plt.title(f'Box Plot for {column}')\n    plt.xlabel(column)\n    plt.show()"
  },
  {
    "objectID": "posts/Tutorial/index.html#visualization-with-seaborn-and-scatterplots",
    "href": "posts/Tutorial/index.html#visualization-with-seaborn-and-scatterplots",
    "title": "Data Visualization Tutorial",
    "section": "Visualization with Seaborn and Scatterplots",
    "text": "Visualization with Seaborn and Scatterplots\nWe can also use a software seaborn, a data visualization tool, help with our data.\n\nimport seaborn as sns\n\nWe can use seaborn to display scatter plots that compare each of the columns againist each other in order to find certain trends.\n\nsns.pairplot(df_cleaned[columns_of_interest])\nplt.suptitle('Scatter Plots for Culmen Length, Culmen Depth, Flipper Length, and Body Mass', y=1.02)\nplt.show()\n\n\n\n\nAs we can see there are a total of 16 plots shown, and when we have a case of the column vs itself, a bar graph is displayed which shows the amount of category. From these charts we can make some insights. For example we can see that there is a relationship between flipper length and body mass. From the scatter plot, we can see that as flipper length increases, so does the body mass."
  },
  {
    "objectID": "posts/Tutorial/index.html#conclusion",
    "href": "posts/Tutorial/index.html#conclusion",
    "title": "Data Visualization Tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nThank you for reading this post!"
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html",
    "href": "posts/Country Temps Visualization/index.html",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "Hello! Welcome to this blog post where I will go over how to use databases for data visualization. For this post we will be using example data of temperature data from various different countries.\n\n\nFirst we need to run the following line of code:\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nFor this post, we are going to be using SQL queries in order to retrevie our data. Then we are going to use some data visualization software to answer a few questions. We will be using sqlite to establish a database and to retreive our data.\n\n\n\nIn a seperate file, I already loaded in the data into a database named “database.db”. Thus we will connect to this database in order to get information about our data. The data we are using consists of three different tables: stations, countries, and temperatures. This data has been collected and shows the temperature readings of different stations in various countries over the years.\nWe will use this data to answer our first question: How does the average yearly change in temperature vary within a given country?\nThus we need to use an SQL query to acess the data that is useful for answering this question.\nWe will also need to include the headers necessary for the rest of the post.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nHere is the query we will use. The function allows a user a to input a database file, country, year_begin, year_end, and month to find the data needed to answer our question.\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n            SELECT\n                stations.NAME,\n                stations.LATITUDE,\n                stations.LONGITUDE,\n                countries.Name AS Country,\n                temperatures.Year,\n                temperatures.Month,\n                temperatures.Temp\n            FROM\n                temperatures\n            JOIN\n                stations ON temperatures.ID = stations.ID\n            JOIN\n                countries ON SUBSTR(stations.ID, 1, 2) = SUBSTR(countries.\"FIPS 10-4\", 1, 2)\n            WHERE\n                countries.Name = '{country}'\n                AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n                AND temperatures.Month = {month}\n        \"\"\"\n\n        # Execute the query and read the results into a DataFrame\n        result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return result_df\n\nNow in our dataframe we have the data that we will use the answer our question: the station’s name, latitude, and longitude, the country’s name, and the tempature of the country and the month we are looking at. This data is stored in result_df.\nThus, we can now write a fucntion, temperature_coefficient_plot to use the pulled data and make a plot. The goal of this function is to create a plot of the selected country with a scatter of data points located at the stations used. Each datapoint will store information about the average yearly change in temperature for the associated station.\nThe function uses the same variables as the previous function, in addition to mins_obs, zoom, mapbox_style, and color_continuous_scale. mins_obs is an inputed variable that only shows stations that have at least mins_obs years of observatoin. The other variables, zoom, mapbox_style, and color_continuous_scale, are for graphing preference of the plot.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, zoom =2, mapbox_style=\"carto-positron\", color_continuous_scale=px.colors.diverging.RdGy_r ):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    grouped_df = df.groupby(\"NAME\")\n\n    dfs_dict = {name: group for name, group in grouped_df}\n\n    filtered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    results = pd.DataFrame()\n    for name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\n    fig = px.scatter_mapbox(\n    results,\n    lat=\"LATITUDE\",\n    lon=\"LONGITUDE\",\n    text=\"NAME\",\n    color=\"EXPECTED_CHANGE\",\n    color_continuous_scale=color_continuous_scale,\n    size_max=30,\n    zoom=zoom,\n    mapbox_style=mapbox_style,\n    title=f\"Expected Temperature Change in {country} ({year_begin}-{year_end}), Month {month}\",\n    )\n\n    fig.show()\n\nThe function first pulls the df using the query_climate_database function, then groups the results by station name.\nThis is what the data frame would look like after being grouped.\n\n#Example data\n(db_file, country, year_begin, year_end, month, min_obs) = (\"database.db\", \"India\", 1980, 2020, 1, 10)\n\ndf = query_climate_database(db_file, country, year_begin, year_end, month)\n\ngrouped_df = df.groupby(\"NAME\")\n\ngrouped_df.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3145\nDARJEELING\n27.050\n88.270\nIndia\n1981\n1\n6.50\n\n\n3146\nDARJEELING\n27.050\n88.270\nIndia\n1982\n1\n8.70\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n\n\n516 rows × 7 columns\n\n\n\nNext, the function creates a seperate dataframe for each station that contains all that stations data. It also filters out any statoins with less than mins_obs years of data. It will store these dataframes in a dictionary where the key will be the name of the station.\nNow that we have the dictionary of station’s data, we use linear regression from the scikit.learn libray, to fit a line of best fit for each station. Thus, the slope of this line will be the rate of change for the temperature at the given station. We will put each station and its rate of change in a seperate dataframe along with the station longitude and latitude. This information will be stored in a dataframe called results.\nSo the list of the rates of change will look like the following.\n\n# create dictionary for dataframes, each keyed by the unique \"NAME\"\ndfs_dict = {name: group for name, group in grouped_df}\n\nfiltered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n#calculate expected change in temp for each station\ndef coef(data_group):\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nresults = pd.DataFrame()\n\nfor name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\nresults\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nEXPECTED_CHANGE\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\n-0.006184\n\n\n1\nAHMADABAD\n23.067\n72.633\n0.006731\n\n\n2\nAKOLA\n20.700\n77.033\n-0.008063\n\n\n3\nALLAHABAD\n25.441\n81.735\n-0.029375\n\n\n4\nALLAHABAD_BAMHRAULI\n25.500\n81.900\n-0.015457\n\n\n...\n...\n...\n...\n...\n\n\n92\nTRIVANDRUM\n8.500\n77.000\n0.022892\n\n\n93\nUDAIPUR_DABOK\n24.617\n73.883\n0.072424\n\n\n94\nVARANASI_BABATPUR\n25.450\n82.867\n-0.012996\n\n\n95\nVERAVAL\n20.900\n70.367\n0.024848\n\n\n96\nVISHAKHAPATNAM\n17.717\n83.233\n-0.034050\n\n\n\n\n97 rows × 4 columns\n\n\n\nFinally, using plotly, we will graph this data on the plot of the chosen country. The function temperature_coefficient_plot has the variables, zoom, mapbox_style, and color_continuous_scale, predefined. Thus a user can choose to input their own prefrences for these variables but if not it will be set to the setting chosen in the function declaration.\nLet’s use our new function to look a plot of an example country. For example lets look at the country of India from years 1980-2020, during the month of January. For this example, we will use a min_obs of 10 years and use the default settings for the plotly graph.\n\n# Example variables\n(country, year_begin, year_end, month, min_obs) = (\"India\", 1980, 2020, 1, 10)\n\n# Call function\ntemperature_coefficient_plot(\"database.db\", country, year_begin, year_end, month, min_obs)\n\n\n\n\nAs we can see an interactive graph was produced by our function. We can zoom in and out on the graph and we are also able to hover over the datapoints to see what the expected rate of change of temperature will be. We can also see that there is a scale on the right that shows how the color of the data point correlates to the rate of change."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#set-up",
    "href": "posts/Country Temps Visualization/index.html#set-up",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "First we need to run the following line of code:\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'iframe'\n\n\nFor this post, we are going to be using SQL queries in order to retrevie our data. Then we are going to use some data visualization software to answer a few questions. We will be using sqlite to establish a database and to retreive our data."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#first-question-and-query",
    "href": "posts/Country Temps Visualization/index.html#first-question-and-query",
    "title": "Data Visualization and Databases",
    "section": "",
    "text": "In a seperate file, I already loaded in the data into a database named “database.db”. Thus we will connect to this database in order to get information about our data. The data we are using consists of three different tables: stations, countries, and temperatures. This data has been collected and shows the temperature readings of different stations in various countries over the years.\nWe will use this data to answer our first question: How does the average yearly change in temperature vary within a given country?\nThus we need to use an SQL query to acess the data that is useful for answering this question.\nWe will also need to include the headers necessary for the rest of the post.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom sklearn.linear_model import LinearRegression\nimport plotly.express as px\n\nHere is the query we will use. The function allows a user a to input a database file, country, year_begin, year_end, and month to find the data needed to answer our question.\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n            SELECT\n                stations.NAME,\n                stations.LATITUDE,\n                stations.LONGITUDE,\n                countries.Name AS Country,\n                temperatures.Year,\n                temperatures.Month,\n                temperatures.Temp\n            FROM\n                temperatures\n            JOIN\n                stations ON temperatures.ID = stations.ID\n            JOIN\n                countries ON SUBSTR(stations.ID, 1, 2) = SUBSTR(countries.\"FIPS 10-4\", 1, 2)\n            WHERE\n                countries.Name = '{country}'\n                AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n                AND temperatures.Month = {month}\n        \"\"\"\n\n        # Execute the query and read the results into a DataFrame\n        result_df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return result_df\n\nNow in our dataframe we have the data that we will use the answer our question: the station’s name, latitude, and longitude, the country’s name, and the tempature of the country and the month we are looking at. This data is stored in result_df.\nThus, we can now write a fucntion, temperature_coefficient_plot to use the pulled data and make a plot. The goal of this function is to create a plot of the selected country with a scatter of data points located at the stations used. Each datapoint will store information about the average yearly change in temperature for the associated station.\nThe function uses the same variables as the previous function, in addition to mins_obs, zoom, mapbox_style, and color_continuous_scale. mins_obs is an inputed variable that only shows stations that have at least mins_obs years of observatoin. The other variables, zoom, mapbox_style, and color_continuous_scale, are for graphing preference of the plot.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, zoom =2, mapbox_style=\"carto-positron\", color_continuous_scale=px.colors.diverging.RdGy_r ):\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    grouped_df = df.groupby(\"NAME\")\n\n    dfs_dict = {name: group for name, group in grouped_df}\n\n    filtered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    results = pd.DataFrame()\n    for name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\n    fig = px.scatter_mapbox(\n    results,\n    lat=\"LATITUDE\",\n    lon=\"LONGITUDE\",\n    text=\"NAME\",\n    color=\"EXPECTED_CHANGE\",\n    color_continuous_scale=color_continuous_scale,\n    size_max=30,\n    zoom=zoom,\n    mapbox_style=mapbox_style,\n    title=f\"Expected Temperature Change in {country} ({year_begin}-{year_end}), Month {month}\",\n    )\n\n    fig.show()\n\nThe function first pulls the df using the query_climate_database function, then groups the results by station name.\nThis is what the data frame would look like after being grouped.\n\n#Example data\n(db_file, country, year_begin, year_end, month, min_obs) = (\"database.db\", \"India\", 1980, 2020, 1, 10)\n\ndf = query_climate_database(db_file, country, year_begin, year_end, month)\n\ngrouped_df = df.groupby(\"NAME\")\n\ngrouped_df.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3145\nDARJEELING\n27.050\n88.270\nIndia\n1981\n1\n6.50\n\n\n3146\nDARJEELING\n27.050\n88.270\nIndia\n1982\n1\n8.70\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n\n\n516 rows × 7 columns\n\n\n\nNext, the function creates a seperate dataframe for each station that contains all that stations data. It also filters out any statoins with less than mins_obs years of data. It will store these dataframes in a dictionary where the key will be the name of the station.\nNow that we have the dictionary of station’s data, we use linear regression from the scikit.learn libray, to fit a line of best fit for each station. Thus, the slope of this line will be the rate of change for the temperature at the given station. We will put each station and its rate of change in a seperate dataframe along with the station longitude and latitude. This information will be stored in a dataframe called results.\nSo the list of the rates of change will look like the following.\n\n# create dictionary for dataframes, each keyed by the unique \"NAME\"\ndfs_dict = {name: group for name, group in grouped_df}\n\nfiltered_dfs_dict = {name: group_df for name, group_df in dfs_dict.items() if len(group_df) &gt;= min_obs}\n\n#calculate expected change in temp for each station\ndef coef(data_group):\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nresults = pd.DataFrame()\n\nfor name, filtered_df in filtered_dfs_dict.items():\n        temp_change = filtered_df.groupby(\"NAME\").apply(coef).reset_index(name='temp_change')\n    \n        latitude = filtered_df[\"LATITUDE\"].iloc[0]\n        longitude = filtered_df[\"LONGITUDE\"].iloc[0]\n\n        result_row = pd.DataFrame({\n            \"NAME\": [name],\n            \"LATITUDE\": [latitude],\n            \"LONGITUDE\": [longitude],\n            \"EXPECTED_CHANGE\": [temp_change[\"temp_change\"].iloc[0]]\n        })\n\n        results = results._append(result_row, ignore_index=True)\n\nresults\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nEXPECTED_CHANGE\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\n-0.006184\n\n\n1\nAHMADABAD\n23.067\n72.633\n0.006731\n\n\n2\nAKOLA\n20.700\n77.033\n-0.008063\n\n\n3\nALLAHABAD\n25.441\n81.735\n-0.029375\n\n\n4\nALLAHABAD_BAMHRAULI\n25.500\n81.900\n-0.015457\n\n\n...\n...\n...\n...\n...\n\n\n92\nTRIVANDRUM\n8.500\n77.000\n0.022892\n\n\n93\nUDAIPUR_DABOK\n24.617\n73.883\n0.072424\n\n\n94\nVARANASI_BABATPUR\n25.450\n82.867\n-0.012996\n\n\n95\nVERAVAL\n20.900\n70.367\n0.024848\n\n\n96\nVISHAKHAPATNAM\n17.717\n83.233\n-0.034050\n\n\n\n\n97 rows × 4 columns\n\n\n\nFinally, using plotly, we will graph this data on the plot of the chosen country. The function temperature_coefficient_plot has the variables, zoom, mapbox_style, and color_continuous_scale, predefined. Thus a user can choose to input their own prefrences for these variables but if not it will be set to the setting chosen in the function declaration.\nLet’s use our new function to look a plot of an example country. For example lets look at the country of India from years 1980-2020, during the month of January. For this example, we will use a min_obs of 10 years and use the default settings for the plotly graph.\n\n# Example variables\n(country, year_begin, year_end, month, min_obs) = (\"India\", 1980, 2020, 1, 10)\n\n# Call function\ntemperature_coefficient_plot(\"database.db\", country, year_begin, year_end, month, min_obs)\n\n\n\n\nAs we can see an interactive graph was produced by our function. We can zoom in and out on the graph and we are also able to hover over the datapoints to see what the expected rate of change of temperature will be. We can also see that there is a scale on the right that shows how the color of the data point correlates to the rate of change."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#question-3-how-does-the-temperature-vary-across-different-latitudes-within-a-specific-country-for-a-given-month",
    "href": "posts/Country Temps Visualization/index.html#question-3-how-does-the-temperature-vary-across-different-latitudes-within-a-specific-country-for-a-given-month",
    "title": "Data Visualization and Databases",
    "section": "Question 3: How does the temperature vary across different latitudes within a specific country for a given month?",
    "text": "Question 3: How does the temperature vary across different latitudes within a specific country for a given month?\nThis next question we try to answer is intruiging as we can see if there is a correlation between latitude and temperature for a country within a given month.\nWe first must define our new query function, query_temperature_by_latitude, which requires a user to input a county and a month to be visualized.\nThe query returns the name and latitude of the station as well as the average temperature for the selected station.\n\ndef query_temperature_by_latitude(db_file, country, month):\n    with sqlite3.connect(db_file) as conn:\n        cmd = f\"\"\"\n            SELECT\n                stations.NAME,\n                stations.LATITUDE,\n                AVG(temperatures.Temp) AS AvgTemperature\n            FROM\n                temperatures\n            JOIN\n                stations ON temperatures.ID = stations.ID\n            JOIN\n                countries ON SUBSTR(stations.ID, 1, 2) = SUBSTR(countries.\"FIPS 10-4\", 1, 2)\n            WHERE\n                countries.Name = '{country}'\n                AND temperatures.Month = {month}\n            GROUP BY\n                stations.NAME, stations.LATITUDE\n            ORDER BY\n                stations.LATITUDE\n        \"\"\"\n\n        result_df = pd.read_sql_query(cmd, conn)\n\n    return result_df\n\nLet’s take a look at what this resulting data frame will look like, using the United States and the month of March.\n\ndf = query_temperature_by_latitude(db_file, \"United States\", 3)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nAvgTemperature\n\n\n\n\n0\nNAALEHU_14\n19.0614\n21.607500\n\n\n1\nKIOLAKAA_7\n19.0667\n20.521212\n\n\n2\nSOUTH_KONA_2232\n19.0825\n18.292609\n\n\n3\nSEA_MTN_1215\n19.1336\n22.496250\n\n\n4\nPAHALA_21\n19.1986\n19.954773\n\n\n...\n...\n...\n...\n\n\n12548\nALPINE\n70.3464\n-22.067143\n\n\n12549\nCOLVILLE_VILLAGE\n70.4322\n-25.835909\n\n\n12550\nWAINWRIGHT_AP\n70.6392\n-24.802381\n\n\n12551\nBARROW_POST_ROGERS_AP\n71.2833\n-25.335980\n\n\n12552\nBARROW_4_ENE\n71.3214\n-23.970000\n\n\n\n\n12553 rows × 3 columns\n\n\n\nNext, we define our function plot_temperature_by_latitude, that will give us the scatter plot of all the stations used.\nThe function again uses the plotly library to produce the graph.\n\ndef plot_temperature_by_latitude(db_file, country, month):\n    result_df = query_temperature_by_latitude(db_file, country, month)\n\n    if result_df.empty:\n        print(\"No data available for the specified parameters.\")\n        return\n\n    fig = px.scatter(\n        result_df,\n        x=\"LATITUDE\",\n        y=\"AvgTemperature\",\n        color=\"AvgTemperature\",\n        size_max=20, \n        title=f\"Temperature Variation by Latitude in {country}, Month {month}\",\n        labels={\"LATITUDE\": \"Latitude\", \"AvgTemperature\": \"Average Temperature\"},\n        hover_name=\"NAME\",\n    )\n\n    fig.show()\n\nNow that we have defined our plotting function, let’s plot the temperatures in the United States and see if we can find any interesting trends.\n\nplot_temperature_by_latitude(db_file, \"United States\", 3)\n\n\n\n\nAs we can see in the plot, there is a clear trend in the United States. As latitude inrcreaes, the average temperature decreases. This intuitively makes sense as we see that the stations in Hawaii are the warmest, while the stations in Alaska are the coldest. This placement is correct as we know that Hawaii is one of the hotest states and Alaska one of the coldest.\nThe graph is also interactive as the user is able to hover over each scatterpoint on the graph and check its latitude and average temperature. There is also a color coded scale on the right that shows the warmer stations as warm colors (yellow, orange) and the colder staions as cold colors (purple, blue)."
  },
  {
    "objectID": "posts/Country Temps Visualization/index.html#conclusion",
    "href": "posts/Country Temps Visualization/index.html#conclusion",
    "title": "Data Visualization and Databases",
    "section": "Conclusion",
    "text": "Conclusion\nIn conlusion, we can see how helpful SQL and graphing software like plotly can be for data visualization. I hope that you enjoyed reading my blog post and learned something interesting!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arvin Hosseini’s Blog",
    "section": "",
    "text": "Data Visualization and Databases\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization and Databases\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization Tutorial\n\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nArvin Hosseini\n\n\n\n\n\n\nNo matching items"
  }
]